{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d314f8d8",
      "metadata": {
        "id": "d314f8d8"
      },
      "source": [
        "# Hands-on Workshop: AI Agents with Tool Use using Gemini\n",
        "\n",
        "Welcome to this hands-on workshop! In this notebook, we will explore how to build an AI Agent capable of using tools (Function Calling) using Google's Gemini API.\n",
        "\n",
        "# Introduction:\n",
        "In the context of agentic AI, tools are external capabilities the LLM can invoke, for example:\n",
        "*   APIs\n",
        "*   Database queries\n",
        "*   Internal services\n",
        "*   Third-party systems\n",
        "*   Internal functions written in code\n",
        "\n",
        "They turn the LLM from something that just talks into something that can act.\n",
        "\n",
        "Remember, LLMs on their own are stateless, have no access to real-time systems, and can’t take action.\n",
        "\n",
        "# But Give Them Tools, and They Can:\n",
        "*  Fetch data from your internal systems\n",
        "*  Trigger events (e.g., send an email, create a JIRA ticket)\n",
        "*  Access structured data like calendars, dashboards, or CRMs\n",
        "*  Run pre-written logic based on business rules\n",
        "\n",
        "*This is how generation turns into execution.*\n",
        "\n",
        "## What we will build\n",
        "We will create a simple agent that can:\n",
        "1. Answer general questions.\n",
        "2. Perform mathematical calculations using a defined tool.\n",
        "3. Retrieve \"live\" information (mocked) using a defined tool.\n",
        "\n",
        "# Why Tools Matter\n",
        "\n",
        "1.   **They unlock execution**: Without tools, your agent is just an assistant that makes suggestions. With tools, it can complete $workflows^*$ end-to-end.\n",
        "2.   **They increase precision**: Rather than hallucinating, the LLM can ask the right system directly — “What’s the actual order status?” instead of making up a delay reason.\n",
        "3. **They let you control risk**: You define what’s exposed. The LLM can’t do anything outside of the tools you register.\n",
        "4. **They enable composability**: If you want to combine your CRM, calendar, and email stack into one assistant, you can expose each of those as tools and let the LLM orchestrate them.\n",
        "\n",
        "## Prerequisites\n",
        "You need a Google AI Studio API Key. If you don't have one, get it here: https://aistudio.google.com/app/apikey\n",
        "\n",
        "Make sure you add it in the colab secrets manager.\n",
        "\n",
        "\n",
        "* A workflow is a recipe an agent uses for task completion. It is a structured, predefined sequence of multiple steps and uses tools to take decisions autonomously. Example: Travel booking, data extraction, sending birthday emails. ***n8n*** is the platform that hosts and runs workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c8b57d",
      "metadata": {
        "id": "e1c8b57d"
      },
      "source": [
        "What is Agentic AI, and how can we use it?\n",
        "\n",
        "Most AI we encounter today is what we call \"passive\" or \"reactive\" AI. It responds to our prompts and requests. Think of asking your smart speaker to play a song or a chatbot answering your customer service questions. They’re helpful, sure, but they’re fundamentally waiting for us to tell them what to do.\n",
        "\n",
        "Agentic AI, on the other hand, takes a fundamentally different approach. It’s all about giving AI the ability to proactively pursue goals, make decisions, and take actions in the world, even without explicit instructions every step of the way.\n",
        "\n",
        "## 0. Setup and Configuration\n",
        "First, let's install and import the library and configure your API key.\n",
        "\n",
        "Recommendation: Please use the Colab Secrets Manager\n",
        "\n",
        "[Student Hit with a 55,444.78$ bill for leaking Gemini API Key](https://www.reddit.com/r/googlecloud/comments/1noctxi/student_hit_with_a_5544478_google_cloud_bill/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c9342f",
      "metadata": {
        "id": "f5c9342f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3275d3ff-ab6d-43ef-d197-0aa071643c29",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-ai-generativelanguage in /usr/local/lib/python3.12/dist-packages (0.6.15)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (2.28.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage) (2.43.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage) (5.29.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary library\n",
        "!pip install google-ai-generativelanguage"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the Libraries and Generate a Client Object\n",
        "\n",
        "The Client Object acts as a single entrypoint for various API services, promoting consistency and simplifying credential and configuration management across different API calls."
      ],
      "metadata": {
        "id": "jHyuzhdirfCp"
      },
      "id": "jHyuzhdirfCp"
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "# Get API key from Colab secrets manager\n",
        "GOOGLE_API_KEY = userdata.get('gkey')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "7Lidt1ZsoIlV"
      },
      "id": "7Lidt1ZsoIlV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemma-3-27b-it')\n",
        "response = model.generate_content(\n",
        "    'What is AI? Answer in 100 words'\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "jARxH3RrryzQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "094367e7-f5c4-404a-ab86-9a25a403c78d"
      },
      "id": "jARxH3RrryzQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## What is AI?\n",
            "\n",
            "Artificial Intelligence (AI) broadly refers to the simulation of human intelligence processes by computer systems. This includes learning, reasoning, and self-correction. \n",
            "\n",
            "Essentially, AI aims to create machines that can perform tasks typically requiring human intelligence. It's achieved through techniques like machine learning (where systems learn from data) and deep learning (using artificial neural networks). \n",
            "\n",
            "AI powers things like voice assistants (Siri, Alexa), recommendation systems (Netflix, Spotify), and increasingly, complex applications in healthcare, finance, and transportation. It's not *one* thing, but a diverse field constantly evolving.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd3f171",
      "metadata": {
        "id": "7cd3f171"
      },
      "source": [
        "## 1. Define Tools\n",
        "\n",
        "Tools are simply Python functions that the model can \"call\". We need to define these functions and provide clear docstrings so the model understands when and how to use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f3e01e",
      "metadata": {
        "id": "19f3e01e"
      },
      "outputs": [],
      "source": [
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Adds two numbers.\n",
        "\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "\n",
        "    Returns:\n",
        "        The sum of a and b.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool defined in AI Summit] Calling add({a}, {b})\")\n",
        "    return a + b\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplies two numbers.\n",
        "\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "\n",
        "    Returns:\n",
        "        The product of a and b.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool defined in AI Summit] Calling multiply({a}, {b})\")\n",
        "    return a * b\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the current weather for a given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city.\n",
        "\n",
        "    Returns:\n",
        "        A string describing the weather.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool defined in AI Summit] Calling get_weather('{city}')\")\n",
        "    # Mock data for demonstration\n",
        "    weather_data = {\n",
        "        \"Bangalore\": \"Windy, 17°C\",\n",
        "        \"Kolkata\": \"Windy, 28°C\",\n",
        "        \"Chennai\": \"Sunny, 32°C\",\n",
        "        \"Hyderabad\": \"Sunny, 30°C\"\n",
        "    }\n",
        "    return weather_data.get(city, \"Weather data not available for this city.\")\n",
        "\n",
        "import random\n",
        "def get_stock_price(ticker: str) -> float:\n",
        "    \"\"\"Gets the current stock price for a given ticker symbol.\n",
        "\n",
        "    Args:\n",
        "        ticker: The stock ticker symbol (e.g., 'GOOG', 'AAPL', 'MSFT').\n",
        "\n",
        "    Returns:\n",
        "        A simulated current stock price for the given ticker.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool defined in AI Summit] Calling get_stock_price('{ticker}')\")\n",
        "    # Simulate stock prices for demonstration purposes\n",
        "    mock_prices = {\n",
        "        \"GOOG\": 175.50,\n",
        "        \"AAPL\": 215.25,\n",
        "        \"MSFT\": 450.70,\n",
        "        \"AMZN\": 180.90,\n",
        "        \"NVDA\": 190.00\n",
        "    }\n",
        "    price = mock_prices.get(ticker.upper())\n",
        "    if price:\n",
        "        # Add some randomness to simulate fluctuations\n",
        "        return round(price * (1 + (random.random() - 0.5) * 0.05), 2) # +/- 2.5% fluctuation\n",
        "    else:\n",
        "        return -1.0 # Indicate stock not found\n",
        "\n",
        "def calculate_indian_income_tax(taxable_income: float, regime: str) -> float:\n",
        "    \"\"\"Calculates the income tax for an employee in India based on taxable income and tax regime.\n",
        "    Args:\n",
        "        taxable_income: The total taxable income.\n",
        "        regime: The chosen tax regime ('new' or 'old').\n",
        "    Returns:\n",
        "        The calculated income tax.\n",
        "    Raises:\n",
        "        ValueError: If an invalid tax regime is provided.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool] Calling calculate_indian_income_tax({taxable_income}, '{regime}')\")\n",
        "    tax = 0.0\n",
        "    if regime == 'new':\n",
        "        # Updated New Tax Regime slabs as provided by the user\n",
        "        if taxable_income <= 1200000:\n",
        "            tax = 0\n",
        "        elif taxable_income <= 1600000:\n",
        "            tax = (taxable_income - 1200000) * 0.15\n",
        "        elif taxable_income <= 2000000:\n",
        "            tax = (400000 * 0.15) + (taxable_income - 1600000) * 0.20\n",
        "        elif taxable_income <= 2400000:\n",
        "            tax = (400000 * 0.15) + (400000 * 0.20) + (taxable_income - 2000000) * 0.25\n",
        "        else:\n",
        "            tax = (400000 * 0.15) + (400000 * 0.20) + (400000 * 0.25) + (taxable_income - 2400000) * 0.30\n",
        "    elif regime == 'old':\n",
        "        if taxable_income <= 250000:\n",
        "            tax = 0\n",
        "        elif taxable_income <= 500000:\n",
        "            tax = (taxable_income - 250000) * 0.05\n",
        "        elif taxable_income <= 1000000:\n",
        "            tax = 12500 + (taxable_income - 500000) * 0.20\n",
        "        else:\n",
        "            tax = 112500 + (taxable_income - 1000000) * 0.30\n",
        "    else:\n",
        "        raise ValueError(\"Invalid tax regime. Choose 'new' or 'old'.\")\n",
        "\n",
        "    return tax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Qu696Q8z6q"
      },
      "source": [
        "## 2. View Current Available Models\n",
        "\n",
        "We can use the ListModels to see the list of available models and their supported methods."
      ],
      "id": "-7Qu696Q8z6q"
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "9vC6qdjR88bA",
        "outputId": "13ec3721-f058-4784-ab18-318cd8d2967c"
      },
      "id": "9vC6qdjR88bA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-3-pro-preview\n",
            "models/gemini-3-pro-image-preview\n",
            "models/nano-banana-pro-preview\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/deep-research-pro-preview-12-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "for model in genai.list_models():\n",
        "  if 'generateContent' in model.supported_generation_methods:\n",
        "    pprint.pprint(model)"
      ],
      "metadata": {
        "id": "Dvqqq7GpqW5z",
        "outputId": "b0591e44-c78a-4070-bc89-6a8a910a8528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "Dvqqq7GpqW5z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-2.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash',\n",
            "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
            "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro',\n",
            "      base_model_id='',\n",
            "      version='2.5',\n",
            "      display_name='Gemini 2.5 Pro',\n",
            "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Experimental',\n",
            "      description='Gemini 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash',\n",
            "      description='Gemini 2.0 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite 001',\n",
            "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite',\n",
            "      description='Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-preview-tts',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
            "      display_name='Gemini 2.5 Flash Preview TTS',\n",
            "      description='Gemini 2.5 Flash Preview TTS',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=16384,\n",
            "      supported_generation_methods=['countTokens', 'generateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-tts',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
            "      display_name='Gemini 2.5 Pro Preview TTS',\n",
            "      description='Gemini 2.5 Pro Preview TTS',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=16384,\n",
            "      supported_generation_methods=['countTokens', 'generateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-1b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 1B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 4B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-12b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 12B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-27b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 27B',\n",
            "      description='',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3n-e4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3n E4B',\n",
            "      description='',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3n-e2b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3n E2B',\n",
            "      description='',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Flash Latest',\n",
            "      display_name='Gemini Flash Latest',\n",
            "      description='Latest release of Gemini Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-flash-lite-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Flash-Lite Latest',\n",
            "      display_name='Gemini Flash-Lite Latest',\n",
            "      description='Latest release of Gemini Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Pro Latest',\n",
            "      display_name='Gemini Pro Latest',\n",
            "      description='Latest release of Gemini Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash-Lite',\n",
            "      description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-image-preview',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Nano Banana',\n",
            "      description='Gemini 2.5 Flash Preview Image',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-image',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Nano Banana',\n",
            "      description='Gemini 2.5 Flash Preview Image',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
            "      base_model_id='',\n",
            "      version='Gemini 2.5 Flash Preview 09-2025',\n",
            "      display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
            "      description='Gemini 2.5 Flash Preview Sep 2025',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-09-25',\n",
            "      display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
            "      description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-3-pro-preview',\n",
            "      base_model_id='',\n",
            "      version='3-pro-preview-11-2025',\n",
            "      display_name='Gemini 3 Pro Preview',\n",
            "      description='Gemini 3 Pro Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-3-pro-image-preview',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Nano Banana Pro',\n",
            "      description='Gemini 3 Pro Image Preview',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/nano-banana-pro-preview',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Nano Banana Pro',\n",
            "      description='Gemini 3 Pro Image Preview',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-robotics-er-1.5-preview',\n",
            "      base_model_id='',\n",
            "      version='1.5-preview',\n",
            "      display_name='Gemini Robotics-ER 1.5 Preview',\n",
            "      description='Gemini Robotics-ER 1.5 Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-computer-use-preview-10-2025',\n",
            "      base_model_id='',\n",
            "      version='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      description='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/deep-research-pro-preview-12-2025',\n",
            "      base_model_id='',\n",
            "      version='deepthink-exp-05-20',\n",
            "      display_name='Deep Research Pro Preview (Dec-12-2025)',\n",
            "      description='Preview release (December 12th, 2025) of Deep Research Pro',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Initialize the Model - Prompt Version"
      ],
      "metadata": {
        "id": "Yk6-HIoXVmhW"
      },
      "id": "Yk6-HIoXVmhW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chat session\n",
        "# Initialize Model with Generative AI\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemma-3-27b-it'\n",
        ")\n",
        "# Start a chat session with the model initialized above\n",
        "chat = model.start_chat()"
      ],
      "metadata": {
        "id": "ppTR0Qngt_xt"
      },
      "id": "ppTR0Qngt_xt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def postProcessResponse(response):\n",
        "  # Load the JSON Object String using loads()\n",
        "  payload = json.loads(response.text)\n",
        "  # Extract the function_name from the JSON Parsed Object\n",
        "  fn_name = payload['function_name']\n",
        "  # Extract the arguments from the JSON Parsed Object\n",
        "  args    = payload['arguments']\n",
        "  # Get the function object from the defined global symbols in the notebook.\n",
        "  fn      = globals()[fn_name]\n",
        "  # Call the function with the arguments passed\n",
        "  result  = fn(*args)\n",
        "  #get_stock_price\n",
        "  return result\n",
        "\n",
        "def getOutput(task):\n",
        "  # Define the Prompt Here that will let the LLM output the required tool to call\n",
        "  prompt  = '''\n",
        "  You are a function router. The following are the functions you have in your disposal: -\n",
        "  1) add(a, b): Returns the addition of a and b\n",
        "  2) multiply(a, b): Returns the multiplication of a and b\n",
        "  3) get_weather(city): Returns the current weather for the given city\n",
        "  4) get_stock_price(ticker): Returns the current stock price for the given ticker symbol\n",
        "  5) calculate_indian_income_tax(taxable_income, regime): Calculates the income tax for an employee in India based on taxable income depending on the regime\n",
        "    which can be either 'new' or 'old'.\n",
        "  Given a user request, decide which function to call and its arguments.\n",
        "  Return the result as a single valid JSON object with this exact schema:\n",
        "  {\n",
        "    \"function_name\": \"<string name of the function to call>\",\n",
        "    \"arguments\": [<list of arguments as JSON values>]\n",
        "  }\n",
        "  Requirements:\n",
        "  - Output ONLY the JSON object, with no extra text.\n",
        "  - Keys must be: \"function_name\" and \"arguments\".\n",
        "  - \"arguments\" must be a JSON array.\n",
        "  - Do NOT wrap the JSON in backticks or markdown.\n",
        "  Example:\n",
        "  User request: \"Get the price of Google stock\"\n",
        "  Output:\n",
        "  {\"function_name\": \"get_stock_price\", \"arguments\": [\"GOOG\"]}\n",
        "  Now process this request: ''' + task\n",
        "  # Use the send_message() method of the chat session object to send the prompt\n",
        "  response = chat.send_message(prompt)\n",
        "  # Return the output\n",
        "  return response"
      ],
      "metadata": {
        "id": "kxv9rspGQVTz"
      },
      "id": "kxv9rspGQVTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = getOutput(\"What is the current stock price of NVIDIA(NVDA)?\")\n",
        "print(response.text)\n",
        "print(postProcessResponse(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "YBVC2aCdSlcy",
        "outputId": "8975d402-9bf0-4028-b292-e8c6e61151c5"
      },
      "id": "YBVC2aCdSlcy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"function_name\": \"get_stock_price\", \"arguments\": [\"NVDA\"]}\n",
            "[Tool defined in AI Summit] Calling get_stock_price('NVDA')\n",
            "185.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = getOutput(\"What is the weather like at Bangalore?\")\n",
        "print(response.text)\n",
        "print(postProcessResponse(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "1CpNdAoMX6PI",
        "outputId": "a521db05-089e-47f0-b5a6-22360136c91b"
      },
      "id": "1CpNdAoMX6PI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"function_name\": \"get_weather\", \"arguments\": [\"Bangalore\"]}\n",
            "[Tool defined in AI Summit] Calling get_weather('Bangalore')\n",
            "Windy, 17°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = getOutput(\"Calculate the Indian income tax for a taxable income of 1800000 under the new regime.\")\n",
        "print(response.text)\n",
        "print(postProcessResponse(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GrUiDDy688cN",
        "outputId": "6d4be2fc-3db9-44e1-c00b-e533f518d5c2"
      },
      "id": "GrUiDDy688cN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"function_name\": \"calculate_indian_income_tax\", \"arguments\": [1800000, \"new\"]}\n",
            "[Tool] Calling calculate_indian_income_tax(1800000, 'new')\n",
            "100000.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ea48e78",
      "metadata": {
        "id": "2ea48e78"
      },
      "source": [
        "## 4. Easier Alternative: Pass the Function Names as a list and let the LLM call the appropriate function 😀\n",
        "\n",
        "Now we can send messages to the chat session. If the model decides it needs to use a tool to answer, it will automatically call the Python function and use the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c68a7c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8f258972-18bf-4c39-8c3b-a56b02f1ad9d"
      },
      "source": [
        "# Add the tools to a list\n",
        "tools_list = [add, multiply, get_weather, get_stock_price, calculate_indian_income_tax]\n",
        "# Initialize Model with Generative AI\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-2.5-flash-lite',\n",
        "    tools = tools_list\n",
        ")\n",
        "# Start a chat session with automatic function calling enabled\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ],
      "id": "3c68a7c7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'add' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3036565501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add the tools to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtools_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_weather\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_stock_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_indian_income_tax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize Model with Generative AI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = genai.GenerativeModel(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gemini-2.5-flash-lite'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'add' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"What is the current stock price of Google (GOOG)?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "FokEcOfEKneL",
        "outputId": "f2e49817-9395-4e93-d6d1-a09d977aca2f"
      },
      "id": "FokEcOfEKneL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chat' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1912716389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the current stock price of Google (GOOG)?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chat' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"What is 57 multiplied by 3?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "jUw93a1g06u0",
        "outputId": "b06f9820-7df1-437d-8738-8c06091128b5"
      },
      "id": "jUw93a1g06u0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chat' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-787008920.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is 57 multiplied by 3?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chat' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99adc980",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "99adc980",
        "outputId": "4de7e76a-b5c1-417a-b59f-b51431e13b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tool defined in AI Summit] Calling get_weather('Bangalore')\n",
            "The weather in Bangalore is windy and 17°C.\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"What is the weather like in Bangalore today?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9b9504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "4a9b9504",
        "outputId": "a66cfcc4-ff10-4d87-999b-a7c334fdbc10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tool defined in AI Summit] Calling multiply(1.5, 2.0)\n",
            "[Tool defined in AI Summit] Calling multiply(2.0, 3.0)\n",
            "[Tool defined in AI Summit] Calling add(3.0, 6.0)\n",
            "The total is $9.\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"If I buy 2 apples at $1.5 each and 3 oranges at $2 each, how much is the total?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the tools to a list\n",
        "tools_list = [add, multiply, get_weather, get_stock_price, calculate_indian_income_tax]\n",
        "# Initialize Model with Generative AI\n",
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-2.5-flash',\n",
        "    tools = tools_list\n",
        ")\n",
        "# Start a chat session with automatic function calling enabled\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ],
      "metadata": {
        "id": "q1b3ZDTVf14r"
      },
      "id": "q1b3ZDTVf14r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Calculate the Indian income tax for a taxable income of 1800000 under the new regime.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "VA6ILyp88pHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "860c6848-417e-447d-b3b9-cea0fa2f181f"
      },
      "id": "VA6ILyp88pHl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tool] Calling calculate_indian_income_tax(1800000.0, 'new')\n",
            "The calculated income tax for a taxable income of 18,00,000 under the new regime is 1,00,000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e18d240",
      "metadata": {
        "id": "5e18d240"
      },
      "source": [
        "## 5. Exploring the Conversation History\n",
        "\n",
        "You can inspect the chat history to see how the model called the functions and how the results were fed back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e57293",
      "metadata": {
        "id": "99e57293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d85c6dea-af62-4fb9-ef84-5c74f3a82e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: user\n",
            "Text: Calculate the Indian income tax for a taxable income of 1800000 under the new regime.\n",
            "--------------------\n",
            "Role: model\n",
            "Function Call: calculate_indian_income_tax{'taxable_income': 1800000.0, 'regime': 'new'}\n",
            "--------------------\n",
            "Role: user\n",
            "Function Response: <proto.marshal.collections.maps.MapComposite object at 0x7fd418f61b80>\n",
            "--------------------\n",
            "Role: model\n",
            "Text: The calculated income tax for a taxable income of 18,00,000 under the new regime is 1,00,000.\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "for content in chat.history:\n",
        "    part = content.parts[0]\n",
        "    print(f\"Role: {content.role}\")\n",
        "    if part.function_call:\n",
        "        print(f\"Function Call: {part.function_call.name}{dict(part.function_call.args)}\")\n",
        "    elif part.function_response:\n",
        "         print(f\"Function Response: {part.function_response.response}\")\n",
        "    else:\n",
        "        print(f\"Text: {part.text}\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example to illustrate Plain Vanilla LLM"
      ],
      "metadata": {
        "id": "P47zhA2DJzO1"
      },
      "id": "P47zhA2DJzO1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we are planning to ask the agent about a trip itinerary to Rohtang Pass next weekend.\n",
        "response = chat.send_message(\"Can you build me a 1-Day itinerary for Rohtang Pass for next weekend?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "zK_XFp3jJyec",
        "outputId": "e989cc9a-7083-43cf-c37d-47b0bfe6d272"
      },
      "id": "zK_XFp3jJyec",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2000.05ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1973.21ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2053.96ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 7111.92ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3671.85ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am sorry, I cannot help you with that. I can only answer questions that can be answered using the available tools. I cannot create a travel itinerary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the model gave some hint that these months are not suitable for visiting Rohtang Pass as it is closed due to snow, we want the LLM to think and check if Rohtang Pass is closed. If closed, provide an itinerary of a nearby destination that is open instead.\n",
        "What if the agent can first check the status from Google using an API whether Rohtang Pass is open or not and if it is open then only give us the itinerary, otherwise it suggests nearby places and what we can do.\n",
        "This workflow to the agents is provided using LangChain and LangGraph\n",
        "\n",
        "Watch out for the session by Chinmay and others after lunch 😀"
      ],
      "metadata": {
        "id": "7v25i1dRSe87"
      },
      "id": "7v25i1dRSe87"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real-Life Example - AI Driven Student Email Automation Script from csv content"
      ],
      "metadata": {
        "id": "JJElwKZWwUEl"
      },
      "id": "JJElwKZWwUEl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Attendance Monitoring & Email Generation Using Gemini API\n",
        "\n",
        "This notebook demonstrates an end-to-end workflow for automating attendance monitoring and generating personalized email alerts for students falling below a required attendance threshold. The workflow:\n",
        "\n",
        "    - Accepts CSV content pasted directly in the script or input\n",
        "    - Accepts a natural-language instruction\n",
        "    - Uses OpenAI API to extract:\n",
        "        * Required action\n",
        "        * Attendance condition\n",
        "        * Email body\n",
        "    - Filters students based on attendance threshold\n",
        "    - Sends emails to parents"
      ],
      "metadata": {
        "id": "Zs5i1DB_FGTP"
      },
      "id": "Zs5i1DB_FGTP"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-genai # This installs the Google Generative AI client library.\n",
        "# After installation, you'll typically use genai.configure(api_key=YOUR_API_KEY) to authenticate your access to the Gemini API."
      ],
      "metadata": {
        "id": "B_1zdfI3n_0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f430b13-3230-4626-e7af-f9d022835815"
      },
      "id": "B_1zdfI3n_0o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.53.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai) (2.43.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import json"
      ],
      "metadata": {
        "id": "ax13S5rF3tVN"
      },
      "id": "ax13S5rF3tVN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" An API (Application Programming Interface) is an encrypted string that identifies a\n",
        "    Google Cloud project for quota, billing and monitoring purposes.\"\"\"\n",
        "\n",
        "# GEMINI_API_KEY=\"\""
      ],
      "metadata": {
        "id": "1MB4WjFcFSNb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14cfa85c-a707-4d09-d08f-1ba128017dbb"
      },
      "id": "1MB4WjFcFSNb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' An API (Application Programming Interface) is an encrypted string that identifies a\\n    Google Cloud project for quota, billing and monitoring purposes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configures the client by authenticating it to use google.generativeai library within the session.\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "ZhLuw9FXFZyT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "da0ba0ff-2fa6-429f-f0ab-802cf034bbce"
      },
      "id": "ZhLuw9FXFZyT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GEMINI_API_KEY' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2579201471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Configures the client by authenticating it to use google.generativeai library within the session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGEMINI_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'GEMINI_API_KEY' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt= \"\"\"From the provided dataset, identify all the students whose attendance is below 75%. For each of these students,\n",
        "                    provide the following details in the specified format:\n",
        "                    - Student Name: {Student Name}\n",
        "                    - Roll No.: {Student Number}\n",
        "                    - Parent Email: {Parent Email}m\n",
        "                    - Proctor Email: {Proctor Email}\n",
        "                    - Email Body: {Email Body}\n",
        "\n",
        "                    Compose a professional email adressed to parents or gaurdians of these students. the email should:\n",
        "                    - Begin with a polite greeting and clearly state the purpose of the email.\n",
        "                    - Inform the parents that their ward is falling behind due to attendance.\n",
        "                    - Emphasize the importance of improving attednance immediately to ensure academic success.\n",
        "                    - Be polite yet firm, encouraging parents to actively suport their child's regular attendance.\n",
        "                    - Include the proctor's name and email in the maessage, encouraging the parents to reach out to the\n",
        "                      proctor to schedule an in-person meeting at their earliest mutual convenience.\n",
        "                    - Convey urgency and concern while remaining respectful and professional.\n",
        "                    - End with a call to action, prompting parents to take immediate steps to address the issue.\n",
        "\n",
        "                    Use the following placeholders in the email body:\n",
        "                    - {Student Name} for the student's name.\n",
        "                    - {Roll Number} for the student's roll number.\n",
        "                    - {Attendance Percentage} for the student's attendance percentage.\n",
        "                    - {Proctor's Name} for the proctor's name.\n",
        "                    - {Proctor's email} for the proctor's email.\n",
        "                \"\"\""
      ],
      "metadata": {
        "id": "jasd5pVqFeG6"
      },
      "id": "jasd5pVqFeG6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "    print(m.name)"
      ],
      "metadata": {
        "id": "xnDkF5uMFiWm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "outputId": "5a34d7a2-db7f-4d84-96ed-c6335f422196",
        "collapsed": true
      },
      "id": "xnDkF5uMFiWm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-3-pro-preview\n",
            "models/gemini-3-pro-image-preview\n",
            "models/nano-banana-pro-preview\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# This code initializes a generative AI model from the Google Generative AI library.\n",
        "model = genai.GenerativeModel(\n",
        "    # We will use a free version like Gemini 2.5 Flash.\n",
        "    # It is suitable for tasks such as answering questions and summarizing text.\n",
        "    model_name=\"models/gemini-2.5-flash\",\n",
        "    # The 'system_instruction' argument provides a system prompt to guide the model's behavior.\n",
        "    # This prompt sets the context and instructs the model on how to process the input\n",
        "    # and format its output, e.g., to extract student data and compose emails as defined in 'system_prompt'.\n",
        "    system_instruction=system_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "GE_U8U5KFnDD"
      },
      "id": "GE_U8U5KFnDD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This variable, `csv_content`, holds our student dataset.\n",
        "# For this demonstration, we're embedding the CSV data directly as a multi-line string\n",
        "# instead of loading it from a file. This simplifies the setup for our class example.\n",
        "# Each line represents a student record, and the first line is the header defining the columns.\n",
        "csv_content = \"\"\"Student Roll Number,Student Name,Gender,CGPA,Date of Birth,Courses,Marks in Each Course,Attendance Percentage,Proctor Name,Proctor Email,Parent Email\n",
        "CSAI001,Rohan Sharma,M,8.2,2003-02-11,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"82;78;85;80;75\",88,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI002,Ananya Gupta,F,9.1,2003-07-29,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"90;92;89;94;88\",95,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI003,Arvind Menon,M,7.5,2002-12-09,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"70;72;68;75;65\",72,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI004,Priya Iyer,F,8.8,2003-04-14,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"88;85;90;87;89\",97,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI005,Vikram Reddy,M,6.9,2002-11-30,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"60;58;62;55;57\",68,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI006,Neha Kulkarni,F,9.3,2003-06-18,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"92;94;90;95;91\",99,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI007,Aditya Nair,M,7.8,2003-01-25,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"74;77;79;72;70\",82,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI008,Sana Khan,F,8.5,2002-10-17,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"86;82;84;88;83\",90,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI009,Harshit Verma,M,7.2,2003-05-05,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"68;65;70;72;66\",74,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI010,Divya Suresh,F,8.9,2003-03-09,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"90;87;91;89;88\",96,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI011,Karthik Raman,M,8.1,2003-02-22,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"80;82;81;83;79\",89,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI012,Aishwarya Patil,F,7.9,2002-09-19,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"75;78;74;72;71\",85,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI013,Manoj Sen,M,6.5,2003-07-02,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"55;58;60;52;50\",70,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI014,Simran Kaur,F,9.0,2003-01-11,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"91;93;90;92;89\",98,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI015,Rahul Chatterjee,M,8.4,2002-08-28,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"83;80;85;82;81\",87,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI016,Nisha Mukherjee,F,7.1,2002-06-16,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"65;67;69;70;63\",73,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI017,Ritesh Jain,M,8.6,2003-03-03,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"88;84;86;90;85\",92,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI018,Swati Deshmukh,F,7.4,2002-12-21,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"72;74;70;68;69\",76,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI019,Arjun Pillai,M,9.2,2003-10-10,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"94;91;92;93;95\",99,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI020,Isha Rathod,F,8.0,2003-05-14,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"82;80;78;79;77\",84,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI021,Tejas Gowda,M,7.6,2003-06-07,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"70;72;75;73;68\",71,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI022,Ragini Singh,F,9.4,2003-02-03,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"95;94;96;93;92\",100,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI023,Shivam Malhotra,M,6.8,2002-09-01,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"58;55;60;62;57\",69,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI024,Harini Sekar,F,8.7,2003-11-19,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"87;89;88;90;85\",94,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI025,Rehan Qureshi,M,7.0,2003-01-15,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"65;63;68;70;66\",72,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI026,Anjali Dutta,F,8.3,2003-03-27,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"84;86;83;82;80\",91,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI027,Kunal Thakur,M,7.3,2002-10-05,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"69;72;71;70;68\",74,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI028,Mansi Reddy,F,9.0,2003-07-23,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"92;90;93;91;89\",97,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI029,Amit Dubey,M,6.7,2002-08-12,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"58;60;55;57;59\",65,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI030,Pooja Bansal,F,8.2,2003-04-06,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"82;84;80;83;81\",89,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI031,Nitin Arora,M,7.9,2003-09-14,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"78;76;79;77;75\",77,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI032,Sonali Jadhav,F,8.6,2003-08-30,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"88;86;87;89;85\",93,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI033,Yash Mittal,M,6.9,2002-12-05,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"62;60;64;61;63\",69,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI034,Keerthi Menon,F,9.1,2003-11-27,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"93;94;92;90;91\",98,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI035,Adarsh Jena,M,7.4,2002-07-16,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"70;72;68;69;71\",73,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UEcczB_wFrgo"
      },
      "id": "UEcczB_wFrgo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This `user_prompt` variable is crucial because it's the specific instruction we give to our Generative AI model.\n",
        "# It combines two main parts:\n",
        "# 1.  The `csv_content`: This is the raw student data we defined earlier, which the AI needs to process.\n",
        "# 2.  The Instruction: This tells the AI *what to do* with the data.\n",
        "#\n",
        "# In this case, the instruction is to:\n",
        "#   -  Find all students whose 'Attendance Percentage' is below 75%.\n",
        "#   -  Format the output as a JSON object, specifically with an `action` of \"send_email\",\n",
        "#      an `attendance_threshold` of 75, an `email_body` (which the AI will generate based on the system prompt),\n",
        "#      and a list of `students` who meet the criteria.\n",
        "#\n",
        "# This structured approach helps ensure the AI understands exactly what information to extract\n",
        "# and how to present it, making it easier for our Python code to parse and use the AI's output.\n",
        "user_prompt = f\"\"\"Below is student dataset in CSV format.\n",
        "\n",
        "                  {csv_content}\n",
        "\n",
        "                  Your task:\n",
        "                  Extract all students with attendance < 75% and produce structured JSON:\n",
        "                  {{\n",
        "                      \"action\": \"send_email\",\n",
        "                      \"attendance_threshold\": 75,\n",
        "                      \"email_body\": \"...\",\n",
        "                      \"students\": [ ... ]\n",
        "                  }}\n",
        "              \"\"\""
      ],
      "metadata": {
        "id": "UYZjVtaRFzUY"
      },
      "id": "UYZjVtaRFzUY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line sends the 'user_prompt' (which contains the CSV data and instructions) to the\n",
        "# generative AI model (initialized as 'model'). The model processes this input and\n",
        "# generates a response. The entire response object is stored in the 'response' variable.\n",
        "response = model.generate_content(user_prompt)"
      ],
      "metadata": {
        "id": "vR01H75rF38J"
      },
      "id": "vR01H75rF38J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "oU7CniJTF6Jo"
      },
      "id": "oU7CniJTF6Jo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line extracts the actual text content from the 'response' object generated by the model.\n",
        "# The '.text' attribute accesses the string output, and '.strip()' removes any leading or\n",
        "# trailing whitespace (like newlines or spaces) to get a clean string, which is then\n",
        "# stored in the 'raw_text' variable.\n",
        "raw_text = response.text.strip()"
      ],
      "metadata": {
        "id": "mHcWLIwbF8gB"
      },
      "id": "mHcWLIwbF8gB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_text)"
      ],
      "metadata": {
        "id": "Xv-P3V7cGFFR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "a48a7c48-4009-400f-ec4b-ba8c6e819341"
      },
      "id": "Xv-P3V7cGFFR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'raw_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3613255655.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'raw_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This block of code is designed to clean the 'raw_text' received from the Generative AI model.\n",
        "# The model sometimes wraps its JSON output in markdown code block syntax (e.g., ```json...```).\n",
        "# These wrappers need to be removed before the text can be parsed as a valid JSON object.\n",
        "\n",
        "# Checks if the 'raw_text' string starts with '```'. This indicates the presence of a markdown code block.\n",
        "if raw_text.startswith(\"```\"):\n",
        "    # If it starts with '```', this line removes all leading and trailing backticks.\n",
        "    raw_text = raw_text.strip(\"`\")\n",
        "    # This line specifically removes the 'json' keyword that often follows the opening backticks\n",
        "    # (e.g., ```json). It only replaces the first occurrence to avoid removing 'json' from actual content.\n",
        "    raw_text = raw_text.replace(\"json\", \"\", 1).strip()"
      ],
      "metadata": {
        "id": "9Qp1KZI3GMEJ"
      },
      "id": "9Qp1KZI3GMEJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_text)"
      ],
      "metadata": {
        "id": "LTEBDhNTGQ3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcc1bbd-0009-4338-d726-4662fc12808c"
      },
      "id": "LTEBDhNTGQ3y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"function_name\": \"calculate_indian_income_tax\", \"arguments\": [1800000, \"new\"]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    parsed = json.loads(raw_text)\n",
        "    print(parsed)\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Failed to parse JSON:\", e)\n",
        "    print(\"Raw LLM response:\", raw_text)\n",
        "    raise"
      ],
      "metadata": {
        "id": "uvjzNCmcGWs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a937ada-751a-472c-ad6d-4f572f7d7d0d"
      },
      "id": "uvjzNCmcGWs7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'function_name': 'calculate_indian_income_tax', 'arguments': [1800000, 'new']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    parsed = json.loads(raw_text)\n",
        "    print(parsed)\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Failed to parse JSON:\", e)\n",
        "    print(\"Raw LLM response:\", raw_text)\n",
        "    raise"
      ],
      "metadata": {
        "id": "RA9ewzs0GZYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3570c89a-fbd7-469e-e0e5-d65609f1a1d2"
      },
      "id": "RA9ewzs0GZYu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'function_name': 'calculate_indian_income_tax', 'arguments': [1800000, 'new']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-32fl82WWt2a"
      },
      "id": "-32fl82WWt2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fN-xiAhQMsrm"
      },
      "id": "fN-xiAhQMsrm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}