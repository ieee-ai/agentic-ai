{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d314f8d8",
      "metadata": {
        "id": "d314f8d8"
      },
      "source": [
        "# Hands-on Workshop: AI Agents with Tool Use using Gemini\n",
        "\n",
        "Welcome to this hands-on workshop! In this notebook, we will explore how to build an AI Agent capable of using tools (Function Calling) using Google's Gemini API.\n",
        "\n",
        "# Introduction:\n",
        "In the context of agentic AI, tools are external capabilities the LLM can invoke, for example:\n",
        "*   APIs\n",
        "*   Database queries\n",
        "*   Internal services\n",
        "*   Third-party systems\n",
        "*   Internal functions written in code\n",
        "\n",
        "They turn the LLM from something that just talks into something that can act.\n",
        "\n",
        "Remember, LLMs on their own are stateless, have no access to real-time systems, and can’t take action.\n",
        "\n",
        "# But Give Them Tools, and They Can:\n",
        "*  Fetch data from your internal systems\n",
        "*  Trigger events (e.g., send an email, create a JIRA ticket)\n",
        "*  Access structured data like calendars, dashboards, or CRMs\n",
        "*  Run pre-written logic based on business rules\n",
        "\n",
        "*This is how generation turns into execution.*\n",
        "\n",
        "## What we will build\n",
        "We will create a simple agent that can:\n",
        "1. Answer general questions.\n",
        "2. Perform mathematical calculations using a defined tool.\n",
        "3. Retrieve \"live\" information (mocked) using a defined tool.\n",
        "\n",
        "# Why Tools Matter\n",
        "\n",
        "1.   **They unlock execution**: Without tools, your agent is just an assistant that makes suggestions. With tools, it can complete $workflows^*$ end-to-end.\n",
        "2.   **They increase precision**: Rather than hallucinating, the LLM can ask the right system directly — “What’s the actual order status?” instead of making up a delay reason.\n",
        "3. **They let you control risk**: You define what’s exposed. The LLM can’t do anything outside of the tools you register.\n",
        "4. **They enable composability**: If you want to combine your CRM, calendar, and email stack into one assistant, you can expose each of those as tools and let the LLM orchestrate them.\n",
        "\n",
        "## Prerequisites\n",
        "You need a Google AI Studio API Key. If you don't have one, get it here: https://aistudio.google.com/app/apikey\n",
        "\n",
        "\n",
        "* A workflow is a recipe an agent uses for task completion. It is a structured, predefined sequence of multiple steps and uses tools to take decisions autonomously. Example: Travel booking, data extraction, sending birthday emails. ***n8n*** is the platform that hosts and runs workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5c9342f",
      "metadata": {
        "id": "f5c9342f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47c43ed-d2fe-4bef-b846-ee41ed0b7dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-ai-generativelanguage in /usr/local/lib/python3.12/dist-packages (0.6.15)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (2.28.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage) (2.43.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage) (5.29.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary library\n",
        "!pip install google-ai-generativelanguage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1c8b57d",
      "metadata": {
        "id": "e1c8b57d"
      },
      "source": [
        "## 0. Setup and Configuration\n",
        "First, let's import the library and configure your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74e1daf",
      "metadata": {
        "id": "b74e1daf"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "# Paste your API key here\n",
        "GOOGLE_API_KEY = \"AIzaSyCGnOv6skMiaojuQOe2ocMUpi88y-pKwt4\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd3f171",
      "metadata": {
        "id": "7cd3f171"
      },
      "source": [
        "## 1. Define Tools\n",
        "\n",
        "Tools are simply Python functions that the model can \"call\". We need to define these functions and provide clear docstrings so the model understands when and how to use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f3e01e",
      "metadata": {
        "id": "19f3e01e"
      },
      "outputs": [],
      "source": [
        "def add(a: float, b: float) -> float:\n",
        "    \"\"\"Adds two numbers.\n",
        "\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "\n",
        "    Returns:\n",
        "        The sum of a and b.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool] Calling add({a}, {b})\")\n",
        "    return a + b\n",
        "\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplies two numbers.\n",
        "\n",
        "    Args:\n",
        "        a: The first number.\n",
        "        b: The second number.\n",
        "\n",
        "    Returns:\n",
        "        The product of a and b.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool] Calling multiply({a}, {b})\")\n",
        "    return a * b\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the current weather for a given city.\n",
        "\n",
        "    Args:\n",
        "        city: The name of the city.\n",
        "\n",
        "    Returns:\n",
        "        A string describing the weather.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool] Calling get_weather('{city}')\")\n",
        "    # Mock data for demonstration\n",
        "    weather_data = {\n",
        "        \"New York\": \"Sunny, 25°C\",\n",
        "        \"London\": \"Rainy, 15°C\",\n",
        "        \"Tokyo\": \"Cloudy, 20°C\",\n",
        "        \"San Francisco\": \"Foggy, 18°C\"\n",
        "    }\n",
        "    return weather_data.get(city, \"Weather data not available for this city.\")\n",
        "\n",
        "# List of tools available to the agent\n",
        "tools_list = [add, multiply, get_weather]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Qu696Q8z6q"
      },
      "source": [
        "## 2. View Current Available Models\n",
        "\n",
        "We can use the ListModels to see the list of available models and their supported methods."
      ],
      "id": "-7Qu696Q8z6q"
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "for model in genai.list_models():\n",
        "  pprint.pprint(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9vC6qdjR88bA",
        "outputId": "f7494820-a489-446f-e2a9-94308a97472d"
      },
      "id": "9vC6qdjR88bA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-2.5-pro-preview-03-25',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-03-25',\n",
            "      display_name='Gemini 2.5 Pro Preview 03-25',\n",
            "      description='Gemini 2.5 Pro Preview 03-25',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash',\n",
            "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
            "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-05-06',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-06',\n",
            "      display_name='Gemini 2.5 Pro Preview 05-06',\n",
            "      description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-06-05',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-06-05',\n",
            "      display_name='Gemini 2.5 Pro Preview',\n",
            "      description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro',\n",
            "      base_model_id='',\n",
            "      version='2.5',\n",
            "      display_name='Gemini 2.5 Pro',\n",
            "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-exp',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash Experimental',\n",
            "      description='Gemini 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash',\n",
            "      description='Gemini 2.0 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
            "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite 001',\n",
            "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Gemini 2.0 Flash-Lite',\n",
            "      description='Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-flash-lite-preview',\n",
            "      base_model_id='',\n",
            "      version='preview-02-05',\n",
            "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
            "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/gemini-2.0-pro-exp',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini 2.0 Pro Experimental',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-exp-1206',\n",
            "      base_model_id='',\n",
            "      version='2.5-exp-03-25',\n",
            "      display_name='Gemini Experimental 1206',\n",
            "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-20',\n",
            "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-20',\n",
            "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-05-20',\n",
            "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
            "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-preview-tts',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
            "      display_name='Gemini 2.5 Flash Preview TTS',\n",
            "      description='Gemini 2.5 Flash Preview TTS',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=16384,\n",
            "      supported_generation_methods=['countTokens', 'generateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-pro-preview-tts',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
            "      display_name='Gemini 2.5 Pro Preview TTS',\n",
            "      description='Gemini 2.5 Pro Preview TTS',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=16384,\n",
            "      supported_generation_methods=['countTokens', 'generateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/learnlm-2.0-flash-experimental',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='LearnLM 2.0 Flash Experimental',\n",
            "      description='LearnLM 2.0 Flash Experimental',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-1b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 1B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 4B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-12b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 12B',\n",
            "      description='',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3-27b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3 27B',\n",
            "      description='',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3n-e4b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3n E4B',\n",
            "      description='',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemma-3n-e2b-it',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemma 3n E2B',\n",
            "      description='',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Flash Latest',\n",
            "      display_name='Gemini Flash Latest',\n",
            "      description='Latest release of Gemini Flash',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-flash-lite-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Flash-Lite Latest',\n",
            "      display_name='Gemini Flash-Lite Latest',\n",
            "      description='Latest release of Gemini Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini Pro Latest',\n",
            "      display_name='Gemini Pro Latest',\n",
            "      description='Latest release of Gemini Pro',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-lite',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash-Lite',\n",
            "      description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-image-preview',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Nano Banana',\n",
            "      description='Gemini 2.5 Flash Preview Image',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-image',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Nano Banana',\n",
            "      description='Gemini 2.5 Flash Preview Image',\n",
            "      input_token_limit=32768,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
            "      base_model_id='',\n",
            "      version='Gemini 2.5 Flash Preview 09-2025',\n",
            "      display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
            "      description='Gemini 2.5 Flash Preview Sep 2025',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
            "      base_model_id='',\n",
            "      version='2.5-preview-09-25',\n",
            "      display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
            "      description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-3-pro-preview',\n",
            "      base_model_id='',\n",
            "      version='3-pro-preview-11-2025',\n",
            "      display_name='Gemini 3 Pro Preview',\n",
            "      description='Gemini 3 Pro Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent',\n",
            "                                    'countTokens',\n",
            "                                    'createCachedContent',\n",
            "                                    'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-3-pro-image-preview',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Nano Banana Pro',\n",
            "      description='Gemini 3 Pro Image Preview',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/nano-banana-pro-preview',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Nano Banana Pro',\n",
            "      description='Gemini 3 Pro Image Preview',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=32768,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-robotics-er-1.5-preview',\n",
            "      base_model_id='',\n",
            "      version='1.5-preview',\n",
            "      display_name='Gemini Robotics-ER 1.5 Preview',\n",
            "      description='Gemini Robotics-ER 1.5 Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-computer-use-preview-10-2025',\n",
            "      base_model_id='',\n",
            "      version='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      description='Gemini 2.5 Computer Use Preview 10-2025',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-embedding-exp-03-07',\n",
            "      base_model_id='',\n",
            "      version='exp-03-07',\n",
            "      display_name='Gemini Embedding Experimental 03-07',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-embedding-exp',\n",
            "      base_model_id='',\n",
            "      version='exp-03-07',\n",
            "      display_name='Gemini Embedding Experimental',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=8192,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n",
            "Model(name='models/imagen-4.0-generate-preview-06-06',\n",
            "      base_model_id='',\n",
            "      version='01',\n",
            "      display_name='Imagen 4 (Preview)',\n",
            "      description='Vertex served Imagen 4.0 model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
            "      base_model_id='',\n",
            "      version='01',\n",
            "      display_name='Imagen 4 Ultra (Preview)',\n",
            "      description='Vertex served Imagen 4.0 ultra model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/imagen-4.0-generate-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Imagen 4',\n",
            "      description='Vertex served Imagen 4.0 model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/imagen-4.0-ultra-generate-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Imagen 4 Ultra',\n",
            "      description='Vertex served Imagen 4.0 ultra model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/imagen-4.0-fast-generate-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Imagen 4 Fast',\n",
            "      description='Vertex served Imagen 4.0 Fast model',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predict'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/veo-2.0-generate-001',\n",
            "      base_model_id='',\n",
            "      version='2.0',\n",
            "      display_name='Veo 2',\n",
            "      description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
            "                   'enabled on the associated Google Cloud Platform account. Please visit '\n",
            "                   'https://console.cloud.google.com/billing to enable it.'),\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/veo-3.0-generate-001',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Veo 3',\n",
            "      description='Veo 3',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/veo-3.0-fast-generate-001',\n",
            "      base_model_id='',\n",
            "      version='3.0',\n",
            "      display_name='Veo 3 fast',\n",
            "      description='Veo 3 fast',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/veo-3.1-generate-preview',\n",
            "      base_model_id='',\n",
            "      version='3.1',\n",
            "      display_name='Veo 3.1',\n",
            "      description='Veo 3.1',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/veo-3.1-fast-generate-preview',\n",
            "      base_model_id='',\n",
            "      version='3.1',\n",
            "      display_name='Veo 3.1 fast',\n",
            "      description='Veo 3.1 fast',\n",
            "      input_token_limit=480,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['predictLongRunning'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-2.0-flash-live-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.0 Flash 001',\n",
            "      description='Gemini 2.0 Flash 001',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-live-2.5-flash-preview',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini Live 2.5 Flash Preview',\n",
            "      description='Gemini Live 2.5 Flash Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-live-preview',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 2.5 Flash Live Preview',\n",
            "      description='Gemini 2.5 Flash Live Preview',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=65536,\n",
            "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-native-audio-latest',\n",
            "      base_model_id='',\n",
            "      version='Gemini 2.5 Flash Native Audio Latest',\n",
            "      display_name='Gemini 2.5 Flash Native Audio Latest',\n",
            "      description='Latest release of Gemini 2.5 Flash Native Audio',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-2.5-flash-native-audio-preview-09-2025',\n",
            "      base_model_id='',\n",
            "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
            "      display_name='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
            "      description='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
            "      input_token_limit=131072,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d54389",
      "metadata": {
        "id": "63d54389"
      },
      "source": [
        "## 3. Initialize the Model with Tools\n",
        "\n",
        "We will now initialize the Gemini model and pass our list of tools to it. We'll use `gemini-1.5-flash` for speed and efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72484a9e",
      "metadata": {
        "id": "72484a9e"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-flash-lite-latest',\n",
        "    tools=tools_list\n",
        ")\n",
        "# Start a chat session with automatic function calling enabled\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ea48e78",
      "metadata": {
        "id": "2ea48e78"
      },
      "source": [
        "## 4. Interact with the Agent\n",
        "\n",
        "Now we can send messages to the chat session. If the model decides it needs to use a tool to answer, it will automatically call the Python function and use the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51c3328",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "a51c3328",
        "outputId": "0ec60a1b-483b-4d39-8c24-19e6af9a5e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tool] Calling multiply(57.0, 3.0)\n",
            "171\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"What is 57 multiplied by 3?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99adc980",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "99adc980",
        "outputId": "48e68d8f-a998-411f-9e8a-10b63a665cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tool] Calling get_weather('San Francisco')\n",
            "The weather in San Francisco today is Foggy and 18°C.\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"What is the weather like in San Francisco today?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9b9504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "4a9b9504",
        "outputId": "d137b49c-0b57-4dec-8814-eeb8de037801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tool] Calling multiply(2.0, 1.5)\n",
            "[Tool] Calling multiply(3.0, 2.0)\n",
            "[Tool] Calling add(3.0, 6.0)\n",
            "The total is $9.\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"If I buy 2 apples at $1.5 each and 3 oranges at $2 each, how much is the total?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e18d240",
      "metadata": {
        "id": "5e18d240"
      },
      "source": [
        "## 5. Exploring the Conversation History\n",
        "\n",
        "You can inspect the chat history to see how the model called the functions and how the results were fed back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e57293",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99e57293",
        "outputId": "3768e221-0eac-4625-e60c-f3507a7b303f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: user\n",
            "Text: What is 57 multiplied by 3?\n",
            "--------------------\n",
            "Role: model\n",
            "Function Call: multiply{'a': 57.0, 'b': 3.0}\n",
            "--------------------\n",
            "Role: user\n",
            "Function Response: <proto.marshal.collections.maps.MapComposite object at 0x783b50feb980>\n",
            "--------------------\n",
            "Role: model\n",
            "Text: 171\n",
            "--------------------\n",
            "Role: user\n",
            "Text: What is the weather like in San Francisco today?\n",
            "--------------------\n",
            "Role: model\n",
            "Function Call: get_weather{'city': 'San Francisco'}\n",
            "--------------------\n",
            "Role: user\n",
            "Function Response: <proto.marshal.collections.maps.MapComposite object at 0x783b50fea4e0>\n",
            "--------------------\n",
            "Role: model\n",
            "Text: The weather in San Francisco today is Foggy and 18°C.\n",
            "--------------------\n",
            "Role: user\n",
            "Text: If I buy 2 apples at $1.5 each and 3 oranges at $2 each, how much is the total?\n",
            "--------------------\n",
            "Role: model\n",
            "Function Call: multiply{'a': 2.0, 'b': 1.5}\n",
            "--------------------\n",
            "Role: user\n",
            "Function Response: <proto.marshal.collections.maps.MapComposite object at 0x783b50fe8920>\n",
            "--------------------\n",
            "Role: model\n",
            "Function Call: add{'a': 3.0, 'b': 6.0}\n",
            "--------------------\n",
            "Role: user\n",
            "Function Response: <proto.marshal.collections.maps.MapComposite object at 0x783b50feb980>\n",
            "--------------------\n",
            "Role: model\n",
            "Text: The total is $9.\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "for content in chat.history:\n",
        "    part = content.parts[0]\n",
        "    print(f\"Role: {content.role}\")\n",
        "    if part.function_call:\n",
        "        print(f\"Function Call: {part.function_call.name}{dict(part.function_call.args)}\")\n",
        "    elif part.function_response:\n",
        "         print(f\"Function Response: {part.function_response.response}\")\n",
        "    else:\n",
        "        print(f\"Text: {part.text}\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144891fd",
      "metadata": {
        "id": "144891fd"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "Try adding your own tool! For example:\n",
        "1. Create another function `get_indian_income_tax(taxable_income, regime) and compute the income tax that the individual has to pay.\n",
        "2. Re-initialize the model with the new tool included.\n",
        "3. Ask the agent to calculate the total price including tax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4dc8002"
      },
      "source": [
        "## Define calculate_indian_income_tax tool\n",
        "\n",
        "Define a new Python function `calculate_indian_income_tax(taxable_income: float, regime: str) -> float` that computes the income tax for an employee in India based on the provided taxable income and chosen tax regime ('new' or 'old'). This will involve implementing the tax slab logic for both regimes as per current Indian tax laws.\n"
      ],
      "id": "c4dc8002"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e6c2fc7"
      },
      "source": [
        "def calculate_indian_income_tax(taxable_income: float, regime: str) -> float:\n",
        "    \"\"\"Calculates the income tax for an employee in India based on taxable income and tax regime.\n",
        "\n",
        "    Args:\n",
        "        taxable_income: The total taxable income.\n",
        "        regime: The chosen tax regime ('new' or 'old').\n",
        "\n",
        "    Returns:\n",
        "        The calculated income tax.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an invalid tax regime is provided.\n",
        "    \"\"\"\n",
        "    print(f\"[Tool] Calling calculate_indian_income_tax({taxable_income}, '{regime}')\")\n",
        "\n",
        "    tax = 0.0\n",
        "    if regime == 'new':\n",
        "        # Updated New Tax Regime slabs as provided by the user\n",
        "        if taxable_income <= 1200000:\n",
        "            tax = 0\n",
        "        elif taxable_income <= 1600000:\n",
        "            tax = (taxable_income - 1200000) * 0.15\n",
        "        elif taxable_income <= 2000000:\n",
        "            tax = (400000 * 0.15) + (taxable_income - 1600000) * 0.20\n",
        "        elif taxable_income <= 2400000:\n",
        "            tax = (400000 * 0.15) + (400000 * 0.20) + (taxable_income - 2000000) * 0.25\n",
        "        else:\n",
        "            tax = (400000 * 0.15) + (400000 * 0.20) + (400000 * 0.25) + (taxable_income - 2400000) * 0.30\n",
        "    elif regime == 'old':\n",
        "        if taxable_income <= 250000:\n",
        "            tax = 0\n",
        "        elif taxable_income <= 500000:\n",
        "            tax = (taxable_income - 250000) * 0.05\n",
        "        elif taxable_income <= 1000000:\n",
        "            tax = 12500 + (taxable_income - 500000) * 0.20\n",
        "        else:\n",
        "            tax = 112500 + (taxable_income - 1000000) * 0.30\n",
        "    else:\n",
        "        raise ValueError(\"Invalid tax regime. Choose 'new' or 'old'.\")\n",
        "\n",
        "    return tax"
      ],
      "id": "7e6c2fc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Calculate the Indian income tax for a taxable income of 1800000 under the new regime.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HD40MhWXO_JJ",
        "outputId": "374e92f3-5de0-4d50-900a-60ca26b96d19"
      },
      "id": "HD40MhWXO_JJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am sorry, I cannot fulfill this request. I do not have access to the current Indian income tax structure for calculations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddb298b7",
        "outputId": "32c54a4f-14f1-4cb9-8f41-60b0e72c60e5"
      },
      "source": [
        "tools_list.append(calculate_indian_income_tax)\n",
        "print(\"Updated tools_list with calculate_indian_income_tax function.\")"
      ],
      "id": "ddb298b7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated tools_list with calculate_indian_income_tax function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    model_name='gemini-flash-lite-latest',\n",
        "    tools=tools_list\n",
        ")\n",
        "# Start a chat session with automatic function calling enabled\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ],
      "metadata": {
        "id": "aSBMZsrDMlbt"
      },
      "id": "aSBMZsrDMlbt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "416ac6ba-ba9a-46fe-bdfc-809b510fbe98",
        "id": "dqffhAQpMrV7"
      },
      "source": [
        "response = chat.send_message(\"Calculate the Indian income tax for a taxable income of 1800000 under the new regime.\")\n",
        "print(response.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Tool] Calling calculate_indian_income_tax(1800000.0, 'new')\n",
            "The Indian income tax for a taxable income of 1,800,000 under the new regime is 100,000.\n"
          ]
        }
      ],
      "id": "dqffhAQpMrV7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "285df786",
        "outputId": "5d1b93ae-d117-4a4f-df34-da1a854f4470"
      },
      "source": [
        "for content in chat.history:\n",
        "    part = content.parts[0]\n",
        "    print(f\"Role: {content.role}\")\n",
        "    if part.function_call:\n",
        "        print(f\"Function Call: {part.function_call.name}{dict(part.function_call.args)}\")\n",
        "    elif part.function_response:\n",
        "         print(f\"Function Response: {part.function_response.response}\")\n",
        "    else:\n",
        "        print(f\"Text: {part.text}\")\n",
        "    print(\"-\" * 20)"
      ],
      "id": "285df786",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: user\n",
            "Text: Calculate the Indian income tax for a taxable income of 1800000 under the new regime.\n",
            "--------------------\n",
            "Role: model\n",
            "Function Call: calculate_indian_income_tax{'taxable_income': 1800000.0, 'regime': 'new'}\n",
            "--------------------\n",
            "Role: user\n",
            "Function Response: <proto.marshal.collections.maps.MapComposite object at 0x783b50fe9dc0>\n",
            "--------------------\n",
            "Role: model\n",
            "Text: The Indian income tax for a taxable income of 1,800,000 under the new regime is 100,000.\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example to illustrate Plain Vanilla LLM"
      ],
      "metadata": {
        "id": "P47zhA2DJzO1"
      },
      "id": "P47zhA2DJzO1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we are planning to ask the agent about a trip itinerary to Rohtang Pass next weekend.\n",
        "response = chat.send_message(\"Can you build me a 1-Day itinerary for Rohtang Pass for next weekend?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "zK_XFp3jJyec",
        "outputId": "8ff6f666-5f13-4fce-9f92-e925fc072d3e"
      },
      "id": "zK_XFp3jJyec",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can certainly try to help you plan an itinerary for Rohtang Pass! However, to give you the best possible plan for **next weekend**, I need a little more information:\n",
            "\n",
            "1.  **What is the exact date of \"next weekend\"?** (e.g., Saturday, October 28th and Sunday, October 29th)\n",
            "2.  **Where will you be starting from?** (Most visitors start from Manali.)\n",
            "3.  **What is your priority for the day?** (e.g., Snow activities, just sightseeing, photography, etc.)\n",
            "\n",
            "**Important Note:** Rohtang Pass has seasonal restrictions. It is usually **closed during heavy winter (mid-November to April/May)** due to heavy snowfall. You **must** check the current opening status and obtain a necessary **permit** online well in advance, as entry is restricted and requires a permit even for a day trip.\n",
            "\n",
            "Once I have the date and starting point, I can create a detailed 1-day plan for you!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the model gave some hint that these months are not suitable for visiting Rohtang Pass as it is closed due to snow, we want the LLM to think and check if Rohtang Pass is closed. If closed, provide an itinerary of a nearby destination that is open instead. This workflow to the agents is provided using LangChain and LangGraph"
      ],
      "metadata": {
        "id": "7v25i1dRSe87"
      },
      "id": "7v25i1dRSe87"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coming Up: LangChain and LangGraph Workflows to Fix the above problem\n",
        "\n",
        "02:00 - 03:00: LangChain Worklow and Index by Chinmay Kulkarni"
      ],
      "metadata": {
        "id": "11N_Jiw6Ruuc"
      },
      "id": "11N_Jiw6Ruuc"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, END\n",
        "# 1. Define the \"Brain\" (State)\n",
        "# This dictionary holds the memory of our agent as it hops between nodes.\n",
        "class AgentState(TypedDict):\n",
        "    destination: str\n",
        "    status: str  # \"open\" or \"closed\"\n",
        "    final_plan: str\n",
        "\n",
        "# 2. Define the Tools/Nodes (The Actions)\n",
        "\n",
        "def check_weather_node(state: AgentState):\n",
        "    \"\"\"Simulates checking the official government website\"\"\"\n",
        "    print(f\"\\n🔎 [Node: Checker] Checking official status for {state['destination']}...\")\n",
        "\n",
        "    # SIMULATION: In a real app, this would be a Google Search API call.\n",
        "    # For the demo, we simulate the real-world fact that Rohtang is closed in Nov.\n",
        "    import datetime\n",
        "    current_month = datetime.datetime.now().month\n",
        "\n",
        "    # If it's winter (Nov-April), Rohtang is closed\n",
        "    if current_month >= 11 or current_month <= 4:\n",
        "        print(\"   ⚠️ ALERT: Official reports say Road Closed due to Snow.\")\n",
        "        return {\"status\": \"closed\"}\n",
        "    else:\n",
        "        print(\"   ✅ Status: Road appears open.\")\n",
        "        return {\"status\": \"open\"}\n",
        "\n",
        "def plan_a_node(state: AgentState):\n",
        "    \"\"\"The original plan if everything is fine\"\"\"\n",
        "    print(\"📝 [Node: Plan A] Generating standard itinerary...\")\n",
        "    res = gemini.invoke(f\"Create a 1-day itinerary for {state['destination']} assuming clear roads.\")\n",
        "    return {\"final_plan\": res.content}\n",
        "\n",
        "def plan_b_node(state: AgentState):\n",
        "    \"\"\"The backup plan (The Pivot)\"\"\"\n",
        "    print(\"🔄 [Node: Plan B] Rerouting to Atal Tunnel/Sissu...\")\n",
        "    prompt = f\"\"\"\n",
        "    The user wanted to go to {state['destination']}, but it is CLOSED due to snow.\n",
        "    Create an ALTERNATIVE 1-day itinerary visiting 'Atal Tunnel' and 'Sissu' instead.\n",
        "    Explain why the change was made.\n",
        "    \"\"\"\n",
        "    res = chat.send_message(prompt)\n",
        "    return {\"final_plan\": res.text}\n",
        "\n",
        "# 3. Define the Logic (The Router)\n",
        "def route_decision(state: AgentState) -> Literal[\"plan_a\", \"plan_b\"]:\n",
        "    \"\"\"Decides which path to take based on the 'status' variable\"\"\"\n",
        "    if state[\"status\"] == \"closed\":\n",
        "        return \"plan_b\"\n",
        "    return \"plan_a\"\n",
        "\n",
        "# 4. Build the Graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"checker\", check_weather_node)\n",
        "workflow.add_node(\"plan_a\", plan_a_node)\n",
        "workflow.add_node(\"plan_b\", plan_b_node)\n",
        "\n",
        "# Connect nodes\n",
        "workflow.set_entry_point(\"checker\")\n",
        "\n",
        "# The Conditional Edge: This is the \"Decision Diamond\" in the flowchart\n",
        "workflow.add_conditional_edges(\n",
        "    \"checker\",          # After checker runs...\n",
        "    route_decision,     # ...run this logic function...\n",
        "    {                   # ...and map the result to a node.\n",
        "        \"plan_a\": \"plan_a\",\n",
        "        \"plan_b\": \"plan_b\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"plan_a\", END)\n",
        "workflow.add_edge(\"plan_b\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "# 5. Run the Demo\n",
        "print(\"🚀 STARTING AGENT...\")\n",
        "inputs = {\"destination\": \"Rohtang Pass\"}\n",
        "result = app.invoke(inputs)\n",
        "\n",
        "print(\"\\n\\n📄 FINAL OUTPUT TO USER:\")\n",
        "print(\"=\" * 40)\n",
        "print(result[\"final_plan\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "JmIfW2utKDzV",
        "outputId": "74716786-e6e5-4eea-ac39-188ebe16cae7"
      },
      "id": "JmIfW2utKDzV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 STARTING AGENT...\n",
            "\n",
            "🔎 [Node: Checker] Checking official status for Rohtang Pass...\n",
            "   ⚠️ ALERT: Official reports say Road Closed due to Snow.\n",
            "🔄 [Node: Plan B] Rerouting to Atal Tunnel/Sissu...\n",
            "\n",
            "\n",
            "📄 FINAL OUTPUT TO USER:\n",
            "========================================\n",
            "That's a very common situation! Rohtang Pass is often closed due to snow outside of the main summer season.\n",
            "\n",
            "Since Rohtang Pass is currently **closed due to snow**, an excellent and very popular alternative for a day trip from Manali is to explore the area beyond the **Atal Tunnel** towards **Sissu**. The Atal Tunnel keeps the Lahaul Valley accessible even when Rohtang is closed.\n",
            "\n",
            "Here is a suggested 1-Day Itinerary focusing on the Atal Tunnel and Sissu:\n",
            "\n",
            "---\n",
            "\n",
            "### **Alternative 1-Day Itinerary: Atal Tunnel & Sissu Exploration**\n",
            "\n",
            "This itinerary focuses on breathtaking views, engineering marvels, and the beautiful landscapes of the Lahaul Valley accessible via the Atal Tunnel.\n",
            "\n",
            "| Time | Activity | Description |\n",
            "| :--- | :--- | :--- |\n",
            "| **8:00 AM** | **Start from Manali** | Have an early breakfast and start your journey from Manali. |\n",
            "| **8:45 AM** | **Approach to Atal Tunnel** | Drive towards the South Portal of the Atal Tunnel. |\n",
            "| **9:30 AM** | **Experience the Atal Tunnel** | Drive through the world's longest single-tube, high-altitude tunnel (9.02 km). The tunnel cuts travel time significantly and offers a unique experience as you transition from Kullu Valley to Lahaul Valley. |\n",
            "| **10:00 AM** | **Visit the North Portal Viewpoint** | Spend some time at the North Portal exit for stunning panoramic views of the mountains. |\n",
            "| **10:30 AM** | **Drive to Sissu** | Continue driving towards Sissu (approx. 30-40 minutes from the North Portal). |\n",
            "| **11:15 AM** | **Sissu Waterfall & Viewpoint** | Visit the majestic **Sissu Waterfall**. Depending on the season, the flow will be strong. There are great spots near the riverbank of the Chandra River for photography. |\n",
            "| **1:00 PM** | **Lunch in Sissu** | Enjoy lunch at one of the local eateries in Sissu. The food is simple but hearty, offering local Himachali flavors. |\n",
            "| **2:00 PM** | **Tendtumar/Gondhla Castle (Optional)** | If time permits and you are interested in history, drive a little further towards Gondhla village to see the **Gondhla Fort/Castle**, a traditional eight-story structure. |\n",
            "| **3:30 PM** | **Begin Return Journey** | Start driving back towards the Atal Tunnel. |\n",
            "| **4:30 PM** | **Sohlang/Naggar Stop (Optional)** | On your way back, stop at **Sohlang Nala** (if accessible/open) for some short activities, or divert slightly to visit the beautiful **Naggar Castle** if you skip Gondhla Castle earlier. |\n",
            "| **6:30 PM** | **Arrive Back in Manali** | Return to Manali, concluding your day trip. |\n",
            "\n",
            "---\n",
            "\n",
            "### **Explanation for the Change**\n",
            "\n",
            "**Why Rohtang Pass is Closed:**\n",
            "Rohtang Pass, being at an altitude of approximately 13,050 feet, receives massive amounts of snow from mid-November until late April or early May. The Border Roads Organisation (BRO) closes the road for safety. Attempting to travel there when closed is illegal and extremely dangerous.\n",
            "\n",
            "**Why Atal Tunnel & Sissu is the Best Alternative:**\n",
            "1.  **Accessibility:** The **Atal Tunnel** bypasses the snow-bound section of Rohtang Pass, providing year-round access to the Lahaul Valley.\n",
            "2.  **Scenery:** The landscape on the Lahaul side (past the tunnel) is starkly different, mountainous, and dramatically beautiful—offering a similar high-altitude mountain experience without the risks of the pass itself.\n",
            "3.  **Sissu:** Sissu is a charming village known for its waterfall and the Chandra River, making it a perfect destination for a relaxing day trip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "==========================================================\n",
        "AI-Driven Student Email Automation Script (CSV Content)\n",
        "==========================================================\n",
        "\n",
        "Dependencies:\n",
        "    pip install pandas openai python-dotenv\n",
        "\n",
        "Description:\n",
        "    - Accepts CSV content pasted directly in the script or input\n",
        "    - Accepts a natural-language instruction\n",
        "    - Uses OpenAI API to extract:\n",
        "        * Required action\n",
        "        * Attendance condition\n",
        "        * Email body\n",
        "    - Filters students based on attendance threshold\n",
        "    - Sends emails to parents\n",
        "    - Logs matched students and sent emails\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import io\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load CSV from content\n",
        "# -------------------------------\n",
        "def load_csv_from_content(csv_content: str) -> pd.DataFrame:\n",
        "    try:\n",
        "        df = pd.read_csv(io.StringIO(csv_content))\n",
        "        print(f\"[INFO] Loaded CSV with {len(df)} rows.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Failed to load CSV content:\", e)\n",
        "        raise\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Parse LLM Instruction\n",
        "# -------------------------------\n",
        "def parse_instruction_with_llm(user_prompt: str) -> dict:\n",
        "    client = OpenAI(api_key=\"YOUR_OPENAI_API_KEY_HERE\")  # <-- Replace\n",
        "\n",
        "    system_msg = \"\"\"\n",
        "You are an LLM that extracts structured instructions from user input.\n",
        "\n",
        "Return ONLY valid JSON in the following format:\n",
        "{\n",
        "  \"action\": \"send_email\",\n",
        "  \"attendance_threshold\": <number>,\n",
        "  \"email_body\": \"<string>\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(response.choices[0].message.content)\n",
        "        print(\"[INFO] LLM parsed instruction:\", parsed)\n",
        "        return parsed\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Unable to parse LLM response:\", e)\n",
        "        print(\"Raw LLM output:\", response.choices[0].message.content)\n",
        "        raise\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Filter Students\n",
        "# -------------------------------\n",
        "def filter_students(df: pd.DataFrame, attendance_threshold: float) -> pd.DataFrame:\n",
        "    df[\"Attendance Percentage\"] = pd.to_numeric(df[\"Attendance Percentage\"], errors='coerce')\n",
        "    filtered = df[df[\"Attendance Percentage\"] < attendance_threshold]\n",
        "    print(f\"[INFO] {len(filtered)} students matched attendance < {attendance_threshold}%.\")\n",
        "    return filtered\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Send Emails\n",
        "# -------------------------------\n",
        "def send_email(to_address: str, subject: str, body: str, smtp_settings: dict):\n",
        "    msg = MIMEText(body)\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg[\"From\"] = smtp_settings[\"sender_email\"]\n",
        "    msg[\"To\"] = to_address\n",
        "\n",
        "    try:\n",
        "        with smtplib.SMTP(smtp_settings[\"smtp_server\"], smtp_settings[\"smtp_port\"]) as server:\n",
        "            server.starttls()\n",
        "            server.login(smtp_settings[\"smtp_user\"], smtp_settings[\"smtp_password\"])\n",
        "            server.send_message(msg)\n",
        "            print(f\"[EMAIL SENT] → {to_address}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Could not send email to {to_address}: {e}\")\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Main\n",
        "# -------------------------------\n",
        "def main():\n",
        "    # Paste your CSV content here\n",
        "    csv_content = \"\"\"Student Roll Number,Student Name,Gender,CGPA,Date of Birth,Courses,Marks in Each Course,Attendance Percentage,Proctor Name,Proctor Email,Parent Email\n",
        "CSAI001,Rohan Sharma,M,8.2,2003-02-11,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"82;78;85;80;75\",88,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI002,Ananya Gupta,F,9.1,2003-07-29,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"90;92;89;94;88\",95,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI003,Arvind Menon,M,7.5,2002-12-09,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"70;72;68;75;65\",72,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI004,Priya Iyer,F,8.8,2003-04-14,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"88;85;90;87;89\",97,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI005,Vikram Reddy,M,6.9,2002-11-30,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"60;58;62;55;57\",68,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI006,Neha Kulkarni,F,9.3,2003-06-18,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"92;94;90;95;91\",99,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI007,Aditya Nair,M,7.8,2003-01-25,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"74;77;79;72;70\",82,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI008,Sana Khan,F,8.5,2002-10-17,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"86;82;84;88;83\",90,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI009,Harshit Verma,M,7.2,2003-05-05,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"68;65;70;72;66\",74,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI010,Divya Suresh,F,8.9,2003-03-09,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"90;87;91;89;88\",96,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI011,Karthik Raman,M,8.1,2003-02-22,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"80;82;81;83;79\",89,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI012,Aishwarya Patil,F,7.9,2002-09-19,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"75;78;74;72;71\",85,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI013,Manoj Sen,M,6.5,2003-07-02,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"55;58;60;52;50\",70,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI014,Simran Kaur,F,9.0,2003-01-11,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"91;93;90;92;89\",98,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI015,Rahul Chatterjee,M,8.4,2002-08-28,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"83;80;85;82;81\",87,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI016,Nisha Mukherjee,F,7.1,2002-06-16,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"65;67;69;70;63\",73,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI017,Ritesh Jain,M,8.6,2003-03-03,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"88;84;86;90;85\",92,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI018,Swati Deshmukh,F,7.4,2002-12-21,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"72;74;70;68;69\",76,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI019,Arjun Pillai,M,9.2,2003-10-10,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"94;91;92;93;95\",99,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI020,Isha Rathod,F,8.0,2003-05-14,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"82;80;78;79;77\",84,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI021,Tejas Gowda,M,7.6,2003-06-07,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"70;72;75;73;68\",71,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI022,Ragini Singh,F,9.4,2003-02-03,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"95;94;96;93;92\",100,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI023,Shivam Malhotra,M,6.8,2002-09-01,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"58;55;60;62;57\",69,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI024,Harini Sekar,F,8.7,2003-11-19,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"87;89;88;90;85\",94,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI025,Rehan Qureshi,M,7.0,2003-01-15,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"65;63;68;70;66\",72,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI026,Anjali Dutta,F,8.3,2003-03-27,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"84;86;83;82;80\",91,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI027,Kunal Thakur,M,7.3,2002-10-05,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"69;72;71;70;68\",74,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI028,Mansi Reddy,F,9.0,2003-07-23,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"92;90;93;91;89\",97,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI029,Amit Dubey,M,6.7,2002-08-12,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"58;60;55;57;59\",65,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI030,Pooja Bansal,F,8.2,2003-04-06,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"82;84;80;83;81\",89,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI031,Nitin Arora,M,7.9,2003-09-14,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"78;76;79;77;75\",77,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI032,Sonali Jadhav,F,8.6,2003-08-30,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"88;86;87;89;85\",93,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI033,Yash Mittal,M,6.9,2002-12-05,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"62;60;64;61;63\",69,Dr. S. Krishnan,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI034,Keerthi Menon,F,9.1,2003-11-27,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"93;94;92;90;91\",98,Dr. Anita Rao,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "CSAI035,Adarsh Jena,M,7.4,2002-07-16,\"Machine Learning;Deep Learning;Natural Language Processing;Computer Vision;Reinforcement Learning\",\"70;72;68;69;71\",73,Dr. Meenakshi Ray,arupdas.research.iitm@gmail.com,das12997@gmail.com\n",
        "\"\"\"\n",
        "\n",
        "    df = load_csv_from_content(csv_content)\n",
        "\n",
        "    user_prompt = input(\"\"\"Compose a professional email to the parents or guardians of all students whose attendance is currently below 75%. The email should\n",
        "                           - Clearly inform the parents that their ward is falling behind due to low attendance.\n",
        "                           - Emphasize the importance of improving attendance immediately.\n",
        "                           - Be polite yet firm, encouraging parents to actively support their child’s regular class attendance.\n",
        "                           - Include the proctor’s email in the message and encourage the parents to reach out to the proctor scheduling an in-person meeting\n",
        "                             at their earliest mutual convenience.\n",
        "                           The tone should convey urgency and concern while remaining respectful and professional. The email should clearly prompt parents to\n",
        "                           take action without sounding overly harsh.\n",
        "    \"\"\")\n",
        "\n",
        "    parsed = parse_instruction_with_llm(user_prompt)\n",
        "\n",
        "    if parsed.get(\"action\") != \"send_email\":\n",
        "        print(\"[INFO] No email sending action detected. Exiting.\")\n",
        "        return\n",
        "\n",
        "    attendance_threshold = parsed[\"attendance_threshold\"]\n",
        "    email_body_template = parsed[\"email_body\"]\n",
        "\n",
        "    filtered_df = filter_students(df, attendance_threshold)\n",
        "\n",
        "    smtp_settings = {\n",
        "        \"smtp_server\": \"smtp.example.com\",\n",
        "        \"smtp_port\": 587,\n",
        "        \"smtp_user\": \"your_username_here\",\n",
        "        \"smtp_password\": \"your_password_here\",\n",
        "        \"sender_email\": \"your_sender_email@example.com\"\n",
        "    }\n",
        "\n",
        "    print(\"\\n===== MATCHING STUDENTS =====\")\n",
        "    for _, row in filtered_df.iterrows():\n",
        "        print(f\"{row['Student Name']} ({row['Attendance Percentage']}%)\")\n",
        "\n",
        "    print(\"\\n===== SENDING EMAILS =====\")\n",
        "    for _, row in filtered_df.iterrows():\n",
        "        email_body = (\n",
        "            email_body_template +\n",
        "            f\"\\n\\nStudent Name: {row['Student Name']}\\n\"\n",
        "            f\"Roll Number: {row['Student Roll Number']}\\n\"\n",
        "            f\"Attendance: {row['Attendance Percentage']}%\\n\"\n",
        "            f\"Proctor: {row['Proctor Name']}\\n\"\n",
        "            f\"Proctor Email: {row['Proctor Email']}\\n\"\n",
        "        )\n",
        "\n",
        "        send_email(\n",
        "            to_address=row[\"Parent Email\"],\n",
        "            subject=\"Attendance Notification\",\n",
        "            body=email_body,\n",
        "            smtp_settings=smtp_settings\n",
        "        )\n",
        "\n",
        "    print(\"\\n[INFO] Completed.\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "B_1zdfI3n_0o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "6830b91a-4971-411f-9fe6-62568b9b7d1f"
      },
      "id": "B_1zdfI3n_0o",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loaded CSV with 35 rows.\n",
            "Compose a professional email to the parents or guardians of all students whose attendance is currently below 75%. The email should\n",
            "                           - Clearly inform the parents that their ward is falling behind due to low attendance.\n",
            "                           - Emphasize the importance of improving attendance immediately.\n",
            "                           - Be polite yet firm, encouraging parents to actively support their child’s regular class attendance.\n",
            "                           - Include the proctor’s email in the message and encourage the parents to reach out to the proctor scheduling an in-person meeting\n",
            "                             at their earliest mutual convenience.\n",
            "                           The tone should convey urgency and concern while remaining respectful and professional. The email should clearly prompt parents to\n",
            "                           take action without sounding overly harsh.\n",
            "    How many got less than 75% marks\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_OPE************HERE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1987294919.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1987294919.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\")\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_instruction_with_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"send_email\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1987294919.py\u001b[0m in \u001b[0;36mparse_instruction_with_llm\u001b[0;34m(user_prompt)\u001b[0m\n\u001b[1;32m     56\u001b[0m \"\"\"\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4.1-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1188\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: YOUR_OPE************HERE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hw9zyW7uscYJ"
      },
      "id": "hw9zyW7uscYJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}