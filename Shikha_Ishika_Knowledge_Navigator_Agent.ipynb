{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# `Hands-on: Build a Knowledge Navigator Agent Using Gemini, Tools & RAG`\n",
        "\n",
        "## üîß Step 1 ‚Äî Install Dependencies\n",
        "\n",
        "Install Required Libraries\n",
        "In this first step, we install langchain, Google‚Äôs Gemini integration, and DuckDuckGo Search (for getting information from Website).\n",
        "These packages enable our agent to run tool-based reasoning and search the web."
      ],
      "metadata": {
        "id": "NmF1KdhPNl0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "dJ5DfqU8mTOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-classic\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install langchain-google-genai\n",
        "\n",
        "# for search tool\n",
        "!pip install ddgs\n",
        "\n",
        "# for RAG tool\n",
        "!pip install chromadb\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers\n",
        "!pip install -U langchain-text-splitters\n",
        "\n",
        "!pip install -U youtube-transcript-api\n",
        "!pip install -U langchain-chroma\n",
        "\n"
      ],
      "metadata": {
        "id": "tdEKX9GfONd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 2 ‚Äî Import Core Modules\n",
        "\n",
        "We now import LangChain components for building the agent,\n",
        "along with utilities for web search and date operations.\n",
        "These imports form the foundation for tool creation and model execution."
      ],
      "metadata": {
        "id": "LnHpxyNkPFmF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ihVviR2Nk1j"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
        "from langchain_classic.tools import Tool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from ddgs import DDGS\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_classic.tools import Tool\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    api_key=\"AIzaSyD43Kb31n9T0nbFjNJGCPwSPx_337F5DXU\",\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "\n",
        "# Step 1: Using Transformers based embedding model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "AYCHS2FU-ohm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîë Step 3 ‚Äî Configure Google Gemini API Access\n",
        "\n",
        "To communicate with Gemini models, we must provide an API key from Google AI Studio.\n",
        "Store it securely using environment variables so we never hard-code sensitive credentials directly in code."
      ],
      "metadata": {
        "id": "xOWFZHECPOFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paste your own API Key here\n",
        "GOOGLE_API_KEY = \"AIzaSyD43Kb31n9T0nbFjNJGCPwSPx_337F5DXU\""
      ],
      "metadata": {
        "id": "2gqtZSyRNs0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Step 4 ‚Äî Define Tools: Math + Web Search\n",
        "\n",
        "\n",
        "Define Tools that the Agent Can Use\n",
        "Tools enable Gemini to take actions, not just predict text.\n",
        "Here we define:\n",
        "\n",
        "add_tool ‚Üí Performs accurate addition\n",
        "\n",
        "get_date_tool ‚Üí Returns today‚Äôs date\n",
        "\n",
        "web_search_tool ‚Üí Searches the web, including latest rumors + credibility labels\n",
        "\n",
        "The agent will automatically decide when to call these tools based on user queries."
      ],
      "metadata": {
        "id": "Qurx5N1pPTok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# üß† Memory store\n",
        "# -----------------------\n",
        "memory = {\n",
        "    \"last_result\": None\n",
        "}\n",
        "\n",
        "# Tool to recall memory\n",
        "def recall_memory(_: str) -> str:\n",
        "    \"\"\"Return last result\"\"\"\n",
        "    return str(memory.get(\"last_result\", \"No memory stored yet\"))"
      ],
      "metadata": {
        "id": "MBsW4Of_nlzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tools for websearch\n",
        "\n",
        "def get_domain(url: str) -> str:\n",
        "    try:\n",
        "        return urlparse(url).netloc.replace(\"www.\", \"\")\n",
        "    except:\n",
        "        return \"unknown-source\"\n",
        "\n",
        "\n",
        "def web_search(query: str):\n",
        "    \"\"\"Search the web for recent info including rumors/leaks\"\"\"\n",
        "    results = DDGS().text(query, max_results=6)  # more results for variety\n",
        "\n",
        "    if not results:\n",
        "        return \"No relevant information found.\"\n",
        "\n",
        "    final_output = []\n",
        "    for idx, r in enumerate(results):\n",
        "        title = r.get(\"title\", \"No Title\")\n",
        "        snippet = r.get(\"body\", \"\").strip()\n",
        "        url = r.get(\"href\", \"\")\n",
        "        domain = get_domain(url)\n",
        "\n",
        "        # Rank-based credibility\n",
        "        credibility = \"High\" if idx < 2 else \"Medium\" if idx < 4 else \"Low / Rumor\"\n",
        "\n",
        "        final_output.append(\n",
        "            f\"üîπ **{title}**\\n\"\n",
        "            f\"   üì∞ {snippet}\\n\"\n",
        "            f\"   üåê Source: {domain} | Credibility: {credibility}\\n\"\n",
        "        )\n",
        "    return \"\\n\".join(final_output)"
      ],
      "metadata": {
        "id": "hKtgY0wKn3gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Adding some more trivial tools\n",
        "\n",
        "def add(numbers: str) -> str:\n",
        "    \"\"\"Add two comma-separated integers\"\"\"\n",
        "    a, b = map(int, numbers.split(\",\"))\n",
        "    result = a + b\n",
        "    memory[\"last_result\"] = result\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "def subtract(numbers: str) -> str:\n",
        "    \"\"\"Subtract two comma-separated integers\"\"\"\n",
        "    a, b = map(int, numbers.split(\",\"))\n",
        "    result = a - b\n",
        "    memory[\"last_result\"] = result\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "def multiply(numbers: str) -> str:\n",
        "    \"\"\"Multiply two comma-separated integers\"\"\"\n",
        "    a, b = map(int, numbers.split(\",\"))\n",
        "    result = a * b\n",
        "    memory[\"last_result\"] = result\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "def recall_memory(_: str) -> str:\n",
        "    \"\"\"Return last result\"\"\"\n",
        "    return str(memory.get(\"last_result\", \"No memory stored yet\"))\n",
        "\n",
        "\n",
        "def youtube_rag(video_and_question: str) -> str:\n",
        "    \"\"\"\n",
        "    Provides the transcription of a YouTube video and the answer to a question given to it.\n",
        "    Input format (strict): '<youtube_url> || <question>'\n",
        "    Example: 'https://www.youtube.com/watch?v=dQw4w9WgXcQ || What is the summary?'\n",
        "    \"\"\"\n",
        "    # try:\n",
        "    #     video_url, question = [part.strip() for part in video_and_question.split(\"||\", 1)]\n",
        "    # except Exception:\n",
        "    #     return \"Error: Please call this tool as '<youtube_url> || <your question>'\"\n",
        "\n",
        "    # üëá FIX: Remove parameter label added by the agent\n",
        "    if video_and_question.startswith(\"video_and_question:\"):\n",
        "        video_and_question = video_and_question.split(\":\", 1)[1].strip()\n",
        "\n",
        "    try:\n",
        "        video_url, question = [part.strip() for part in video_and_question.split(\"||\", 1)]\n",
        "    except Exception:\n",
        "        return \"Error: Please call this tool as '<youtube_url> || <your question>'\"\n",
        "\n",
        "\n",
        "    loader = YoutubeLoader.from_youtube_url(video_url)\n",
        "    docs = loader.load()  # Usually one Document with all transcript\n",
        "\n",
        "    print(docs[0:800])\n",
        "\n",
        "    print(\"=================== stage 3: Chunking Stage ===================================\")\n",
        "\n",
        "    # Step 3: Control chunking\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "    )\n",
        "    chunks = splitter.split_documents(docs)\n",
        "    if not chunks:\n",
        "        return \"No transcript found to process.\"\n",
        "\n",
        "    print(\"=================== stage 4: Store the Embeddings in Chroma DB ===================================\")\n",
        "    # Step 4: Build in-memory Chroma DB\n",
        "    vectordb = Chroma.from_documents(\n",
        "        chunks,\n",
        "        embedding=embeddings,\n",
        "    )\n",
        "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "    print(\"=================== stage 5: Retrieve context for RAG ===================================\")\n",
        "    # Step 5: Retrieve context for RAG\n",
        "    relevant_docs = retriever.invoke(question)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in relevant_docs])\n",
        "\n",
        "\n",
        "    print(\"Context retrieved: \", context)\n",
        "\n",
        "    # Step 6: Format prompt and run LLM\n",
        "    print(\"=================== stage 6: Format Prompt and run LLM ===================================\")\n",
        "    rag_prompt = ChatPromptTemplate.from_template(\n",
        "        \"You are a helpful agent. Use the context below to answer the question.\\n\"\n",
        "        \"Context:\\n{context}\\n\\n\"\n",
        "        \"Question: {question}\\n\\n\"\n",
        "        \"If the context isn‚Äôt enough, say you are unsure\"\n",
        "    )\n",
        "    messages = rag_prompt.format_messages(context=context, question=question)\n",
        "    resp = llm.invoke(messages)\n",
        "    return resp.content"
      ],
      "metadata": {
        "id": "P3LeoLHLPZsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è Step 5 ‚Äî Setting up knowledgebase for RAG tools"
      ],
      "metadata": {
        "id": "eaCm98oWoKsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps to upload your documents to create your own knowledgebase\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "\n",
        "# Create a directory to store uploaded PDFs\n",
        "pdf_dir = \"/content/pdfs\"\n",
        "if os.path.exists(pdf_dir):\n",
        "    shutil.rmtree(pdf_dir) # Remove existing directory if necessary\n",
        "os.makedirs(pdf_dir, exist_ok=True)\n",
        "print(f\"Created directory: {pdf_dir}\")\n",
        "\n",
        "# Upload the PDFs from your local machine\n",
        "print(\"Please upload your PDF files now:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move the uploaded files into the created directory\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, os.path.join(pdf_dir, filename))\n",
        "    print(f\"Moved '{filename}' to '{pdf_dir}/'\")\n",
        "\n",
        "print(\"\\nUpload complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "85meHUnITIaX",
        "outputId": "4154a4a6-7947-4517-c2bf-5b83b883a231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: /content/pdfs\n",
            "Please upload your PDF files now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c180666b-176e-4441-9755-8ad639fbc8cb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c180666b-176e-4441-9755-8ad639fbc8cb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving  Clinical Practice Guideline Express Update on the management of metastatic pancreatic cancer.pdf to  Clinical Practice Guideline Express Update on the management of metastatic pancreatic cancer.pdf\n",
            "Saving Clinical Practice Guideline interim update on the treatment of locally advanced oesophageal and oesophagogastric junction adenocarcinoma and metastatic squamous-cell carcinoma.pdf to Clinical Practice Guideline interim update on the treatment of locally advanced oesophageal and oesophagogastric junction adenocarcinoma and metastatic squamous-cell carcinoma.pdf\n",
            "Saving Early and locally advanced non-small-cell lung cancer- ESMO Clinical Practice Guideline for diagnosis, treatment and follow-up.pdf to Early and locally advanced non-small-cell lung cancer- ESMO Clinical Practice Guideline for diagnosis, treatment and follow-up.pdf\n",
            "Saving Localised rectal cancer.pdf to Localised rectal cancer.pdf\n",
            "Saving Lymphomas- ESMO Clinical Practice Guideline for diagnosis, treatment and follow-up.pdf to Lymphomas- ESMO Clinical Practice Guideline for diagnosis, treatment and follow-up.pdf\n",
            "Saving Pan-Asia adapted ESMO Clinical Practice Guideline for the management of patients with newly diagnosed and relapsed epithelial ovarian cancer.pdf to Pan-Asia adapted ESMO Clinical Practice Guideline for the management of patients with newly diagnosed and relapsed epithelial ovarian cancer.pdf\n",
            "Saving Updated treatment recommendations for systemic treatment- from the ESMO Metastatic Breast Cancer Living Guideline.pdf to Updated treatment recommendations for systemic treatment- from the ESMO Metastatic Breast Cancer Living Guideline.pdf\n",
            "Saving Updated treatment recommendations for systemic treatment- from the ESMO non-oncogene-addicted metastatic NSCLC Living Guideline.pdf to Updated treatment recommendations for systemic treatment- from the ESMO non-oncogene-addicted metastatic NSCLC Living Guideline.pdf\n",
            "Saving Updated treatment recommendations for systemic treatment- from the ESMO oncogene-addicted metastatic NSCLC living guideline.pdf to Updated treatment recommendations for systemic treatment- from the ESMO oncogene-addicted metastatic NSCLC living guideline.pdf\n",
            "Moved ' Clinical Practice Guideline Express Update on the management of metastatic pancreatic cancer.pdf' to '/content/pdfs/'\n",
            "Moved 'Clinical Practice Guideline interim update on the treatment of locally advanced oesophageal and oesophagogastric junction adenocarcinoma and metastatic squamous-cell carcinoma.pdf' to '/content/pdfs/'\n",
            "Moved 'Early and locally advanced non-small-cell lung cancer- ESMO Clinical Practice Guideline for diagnosis, treatment and follow-up.pdf' to '/content/pdfs/'\n",
            "Moved 'Localised rectal cancer.pdf' to '/content/pdfs/'\n",
            "Moved 'Lymphomas- ESMO Clinical Practice Guideline for diagnosis, treatment and follow-up.pdf' to '/content/pdfs/'\n",
            "Moved 'Pan-Asia adapted ESMO Clinical Practice Guideline for the management of patients with newly diagnosed and relapsed epithelial ovarian cancer.pdf' to '/content/pdfs/'\n",
            "Moved 'Updated treatment recommendations for systemic treatment- from the ESMO Metastatic Breast Cancer Living Guideline.pdf' to '/content/pdfs/'\n",
            "Moved 'Updated treatment recommendations for systemic treatment- from the ESMO non-oncogene-addicted metastatic NSCLC Living Guideline.pdf' to '/content/pdfs/'\n",
            "Moved 'Updated treatment recommendations for systemic treatment- from the ESMO oncogene-addicted metastatic NSCLC living guideline.pdf' to '/content/pdfs/'\n",
            "\n",
            "Upload complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps to create vector store and set up retriever\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents from the 'pdfs' directory\n",
        "loader = PyPDFDirectoryLoader(pdf_dir)\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "print(f\"Loaded {len(documents)} documents and split into {len(splits)} chunks.\")\n",
        "\n",
        "# Define the embedding model and create the vector store\n",
        "# Using \"text-embedding-004\" as a powerful embedding model\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\", google_api_key= GOOGLE_API_KEY)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "# Configure the retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "print(\"Vector store and retriever created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiTIPjc3TKTt",
        "outputId": "2d7c668f-6ac5-4570-c2a1-4011d6ebafc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 108 documents and split into 605 chunks.\n",
            "Vector store and retriever created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps to create a RAG tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# Helper function to format retrieved documents for the prompt\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    api_key = GOOGLE_API_KEY,\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "# define RAG tool\n",
        "def db_lookup(query: str) -> str:\n",
        "    \"\"\"Return last result\"\"\"\n",
        "    prompt_template = \"\"\"\n",
        "      Answer the question based ONLY on the following context:\n",
        "\n",
        "      {context}\n",
        "\n",
        "      ---\n",
        "\n",
        "      Question: {question}\n",
        "      \"\"\"\n",
        "    rag_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "\n",
        "\n",
        "\n",
        "  # Build the LangChain RAG chain\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | rag_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return str(rag_chain.invoke(query))\n",
        "\n"
      ],
      "metadata": {
        "id": "znF1oB_DpWxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "L15GS1wUVmTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è Step 5 ‚Äî Create Tool Schema for LangChain\n",
        "\n",
        "\n",
        "Convert Functions into Tool Classes\n",
        "LangChain expects tools in a standardized format.\n",
        "Here, we wrap our Python functions into LangChain Tool objects\n",
        "so that the agent can execute them when required."
      ],
      "metadata": {
        "id": "t5rFoSuZPdRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(name=\"add\", func=add, description=\"Adds two numbers, format: x,y\"),\n",
        "    Tool(name=\"subtract\", func=subtract, description=\"Subtract two numbers, format: x,y\"),\n",
        "    Tool(name=\"multiply\", func=multiply, description=\"Multiply two numbers, format: x,y\"),\n",
        "    Tool(name=\"web_search\", func=web_search, description=\"Search the web (DuckDuckGo)\"),\n",
        "    Tool(name=\"recall_memory\", func=recall_memory, description=\"Returns last stored result\"),\n",
        "    Tool(name=\"db_lookup\", func=db_lookup, description=\"Look into this knowledgbase for any cancer related questions\"),\n",
        "    Tool(\n",
        "    name=\"youtube_rag\",\n",
        "    func=youtube_rag,\n",
        "    description=\"Transcribes the video and answers the question from the video\",\n",
        "    args_schema=None  # prevents adding param name in the input\n",
        ")\n",
        "]"
      ],
      "metadata": {
        "id": "pBpZRkYdPa_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Step 6 ‚Äî Create the Agent Prompt\n",
        "\n",
        "Build System Prompt with Rumor Mode Enabled\n",
        "Prompts define your agent‚Äôs personality and instructions.\n",
        "In Rumor Mode, the agent is allowed to:\n",
        "\n",
        "Share latest leaks and speculative info\n",
        "\n",
        "Mark them clearly as rumors\n",
        "\n",
        "Display source credibility"
      ],
      "metadata": {
        "id": "IpFmeuNxRgPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# üîí Strict ReAct Agent Prompt\n",
        "# -----------------------\n",
        "system_message = \"\"\"\n",
        "You are a reliable agent that decides when to call tools.\n",
        "\n",
        "TOOLS AVAILABLE:\n",
        "{tools}\n",
        "Tool names: {tool_names}\n",
        "\n",
        "RULES (must follow EXACTLY)\n",
        "- Use tools ONLY when needed\n",
        "- When using a tool, follow this format EXACTLY:\n",
        "\n",
        "Thought: Should I use a tool? Think step by step\n",
        "Action: <tool name>\n",
        "Action Input: <ONLY the raw input expected by tool>\n",
        "\n",
        "After tool returns result:\n",
        "Observation: <tool output>\n",
        "\n",
        "If the answer is complete:\n",
        "Final Answer: <direct response to user>\n",
        "\n",
        "VERY IMPORTANT:\n",
        "- Do NOT add text inside Action Input that isn‚Äôt part of parameters.\n",
        "- Do NOT calculate results yourself ‚Äî let tools compute.\n",
        "- If referring to old result, use `recall_memory`.\n",
        "- If no tool required: respond with Final Answer directly.\n",
        "- When using youtube_rag: Action Input MUST be exactly '<youtube_url> || <question>' (NO parameter names!)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Create Prompt\n",
        "# -----------------------\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_message + \"\\nTools available: {tools}\\nTool names: {tool_names}\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"assistant\", \"{agent_scratchpad}\")\n",
        "])"
      ],
      "metadata": {
        "id": "ZFHfMfgNPayn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Step 7 ‚Äî Instantiate Gemini LLM\n",
        "\n",
        "Initialize the Gemini LLM\n",
        "We now load Gemini 1.5 Flash through LangChain,\n",
        "which supports function calling + reasoning steps.\n",
        "This model will act as the \"brain\" of the agent."
      ],
      "metadata": {
        "id": "d96VIFNkR0hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    api_key=\"AIzaSyD43Kb31n9T0nbFjNJGCPwSPx_337F5DXU\",\n",
        "    temperature=0.2,\n",
        ")"
      ],
      "metadata": {
        "id": "pchAkPczRz3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Step 8 ‚Äî Create the Agent and Executor\n",
        "\n",
        "\n",
        "Assemble the Tool-Aided Agent\n",
        "We combine:\n",
        "\n",
        "LLM\n",
        "\n",
        "Tools\n",
        "\n",
        "System Prompt\n",
        "into a single AgentExecutor."
      ],
      "metadata": {
        "id": "Xser4fwTR_mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create Agent\n",
        "agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=prompt,\n",
        ")\n",
        "\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")"
      ],
      "metadata": {
        "id": "X6XT0Q9mRzxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Step 9 ‚Äî Test: Tool-Based Math Calculation and Web Search\n",
        "\n",
        "Let‚Äôs confirm the agent uses the add tool\n",
        "instead of guessing the answer.\n",
        "\n",
        "The model should output:\n",
        "\n",
        "Thought\n",
        "\n",
        "Tool selection\n",
        "\n",
        "Final Answer"
      ],
      "metadata": {
        "id": "zNXQGb7iSYd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# üöÄ Test Run\n",
        "# -----------------------\n",
        "\n",
        "queries = [\n",
        "    \"Add 10 and 100 using the tool\",\n",
        "    \"Multiply the last result by 5\",\n",
        "    \"What was my last answer?\",\n",
        "    \"How is weather in Boston today\",   ##Can you give me a list of latest Google Pixel smartphones\"\n",
        "    \"I have a patient diagnosed with ovarian cancer, what investigations should I order?\"\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    print(\"\\nüéØ Query:\", q)\n",
        "    result = agent_executor.invoke({\"input\": q})\n",
        "    print(\"ü§ñ Final Output:\", result[\"output\"])"
      ],
      "metadata": {
        "id": "HauaREh2Rzqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec94893-110d-4666-e5c7-bd140146a115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Query: Add 10 and 100 using the tool\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Should I use a tool? Yes, I need to add two numbers.\n",
            "Action: add\n",
            "Action Input: 10,100\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m110\u001b[0m\u001b[32;1m\u001b[1;3m10 + 100 = 110\n",
            "Final Answer: 110\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ü§ñ Final Output: 110\n",
            "\n",
            "üéØ Query: Multiply the last result by 5\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Should I use a tool?\n",
            "Action: recall_memory\n",
            "Action Input: last stored result\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m110\u001b[0m\u001b[32;1m\u001b[1;3m110 is the last result. I need to multiply it by 5.\n",
            "Action: multiply\n",
            "Action Input: 110,5\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m550\u001b[0m\u001b[32;1m\u001b[1;3m110 multiplied by 5 is 550.\n",
            "Final Answer: 550\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ü§ñ Final Output: 550\n",
            "\n",
            "üéØ Query: What was my last answer?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to recall the last stored result.\n",
            "Action: recall_memory\n",
            "Action Input: last turn\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m550\u001b[0m\u001b[32;1m\u001b[1;3m550 was the last answer.\n",
            "Final Answer: 550\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ü§ñ Final Output: 550\n",
            "\n",
            "üéØ Query: How is weather in Boston today\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: Should I use a tool?\n",
            "Action: web_search\n",
            "Action Input: weather in Boston today\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3189197257.py:12: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  results = DDGS().text(query, max_results=6)  # more results for variety\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3mNo relevant information found.\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Final Answer: I'm sorry, I don't have the current weather information for Boston.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ü§ñ Final Output: I'm sorry, I don't have the current weather information for Boston.\n",
            "\n",
            "üéØ Query: I have a patient diagnosed with ovarian cancer, what investigations should I order?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mThought: I need to find out what investigations are typically ordered for ovarian cancer.\n",
            "Action: db_lookup\n",
            "Action Input: investigations for ovarian cancer\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;200m\u001b[1;3mBased on the context provided, investigations for suspected EOC should include:\n",
            "\n",
            "*   Serum CA-125 measurement\n",
            "*   Pelvic US by an expert examiner\n",
            "*   CT scan of the thorax, abdomen, and pelvis\n",
            "*   Pathological diagnosis according to the 2020 WHO classification by an oncological pathologist, preferably a gynaecological pathologist\n",
            "*   Germline and/or somatic BRCA1/2-mut testing for all patients with high-grade, non-mucinous ovarian cancer at diagnosis\n",
            "*   HRD testing is recommended in advanced high-grade cancers\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m\n",
            "Final Answer: Based on the information I found, investigations for suspected ovarian cancer should include: Serum CA-125 measurement, Pelvic US by an expert examiner, CT scan of the thorax, abdomen, and pelvis, Pathological diagnosis, Germline and/or somatic BRCA1/2-mut testing for all patients with high-grade, non-mucinous ovarian cancer at diagnosis, and HRD testing in advanced high-grade cancers.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "ü§ñ Final Output: Based on the information I found, investigations for suspected ovarian cancer should include: Serum CA-125 measurement, Pelvic US by an expert examiner, CT scan of the thorax, abdomen, and pelvis, Pathological diagnosis, Germline and/or somatic BRCA1/2-mut testing for all patients with high-grade, non-mucinous ovarian cancer at diagnosis, and HRD testing in advanced high-grade cancers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aa3QDeQRRzeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}